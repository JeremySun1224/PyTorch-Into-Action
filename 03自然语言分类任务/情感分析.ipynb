{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Averageing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TorchText中的一个重要概念是Field。Field决定了你的数据会被如何处理；\n",
    "- 我们使用TEXT field来定义如何处理电影评论，使用LABEL field来处理两个情感类别；\n",
    "- LABEL由LabelField定义。这是一种特别的用来处理label的Field。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1224\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(text_field=TEXT, label_field=LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看每个数据splits有多少条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看一个example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label'])\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用.split()创建一个新的validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查每个部分有多少数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocabulary就是把每个单词映射到一个数字。\n",
    "- 我们使用最常见的25k个单词来构建我们的单词表，用max_size可以做到这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\PycharmProjects\\\\PyTorchLearning\\\\PyTorch-Into-Action\\\\03自然语言分类任务'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data, max_size=25000, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 把句子传入模型时，我们是按照一个一个batch传入的。也就是说，我们一次传入了多个句子，而且每个batch中的句子必须是相同的长度。为了确保句子的长度相同，torchtext会把短的句子pad到和最长的句子等长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练数据中最长的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 203961), (',', 193946), ('.', 165993), ('and', 109823), ('a', 109782), ('of', 101067), ('to', 94340), ('is', 76438), ('in', 61518), ('I', 54038)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用 `stoi`(**s**tring **to** **i**nt) 或者 `itos` (**i**nt **to**  **s**tring) 来查看单词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 每个itartion都会返回一个batch的examples。\n",
    "- 我们会使用`BucketIterator`。`BucketIterator`会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。\n",
    "- 严格来说，这份notebook中的模型代码都有一个问题，也就是我们把`<pad>`也当做了模型的输入进行训练。更好的做法是在模型中把由`<pad>`产生的输出给消除掉。在这节课中我们简单处理，直接把`<pad>`也用作模型输入了。由于`<pad>`数量不多，模型的效果也不差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    datasets=(train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 473, 2595,   14,  ..., 1118, 3107,   11],\n",
       "        [  11,   22,    0,  ...,   22,    3,   62],\n",
       "        [ 531,   13,   14,  ...,   17,  403,   27],\n",
       "        ...,\n",
       "        [1655,    6,  145,  ...,    1,    1,    1],\n",
       "        [  85,  273,  145,  ...,    1,    1,    1],\n",
       "        [   4,    4,  145,  ...,    1,    1,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  473,    11,   531,    12,  1639,    33,     6,   378,   100,    67,\n",
       "           32,    78,     6,   189,   715,  6618,  1748,     3,    26,    11,\n",
       "         1764,   119,    12,     9,    88,    53,    15,     6,    63,   337,\n",
       "         1190,    22,     3,  1322, 17296,    86,     6,  5142,   313,    21,\n",
       "           35,   121,     5,    12,    74,  1655,    85,     4],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'it',\n",
       " 'supposedly',\n",
       " 'not',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'because',\n",
       " 'there',\n",
       " 'are',\n",
       " 'only',\n",
       " 'a',\n",
       " 'few',\n",
       " 'easily',\n",
       " 'recognizable',\n",
       " 'faces',\n",
       " ',',\n",
       " 'but',\n",
       " 'I',\n",
       " 'personally',\n",
       " 'think',\n",
       " 'it',\n",
       " 'is',\n",
       " '...',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'very',\n",
       " 'beautiful',\n",
       " 'sweet',\n",
       " 'movie',\n",
       " ',',\n",
       " 'Henry',\n",
       " 'Winkler',\n",
       " 'did',\n",
       " 'a',\n",
       " 'GREAT',\n",
       " 'job',\n",
       " 'with',\n",
       " 'his',\n",
       " 'character',\n",
       " 'and',\n",
       " 'it',\n",
       " 'really',\n",
       " 'impressed',\n",
       " 'me',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TEXT.vocab.itos[i] for i in batch.text[:, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Averaging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        super(WordAVGModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)  # 从__init__()传入\n",
    "        self.linear = nn.Linear(in_features=embedding_dim, out_features=output_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  # text: [seq_len, batch_size, embedding_dim]\n",
    "        # embedded = embedded.transpose(1, 0)  # [batch_size, seq_len, embedding_dim]\n",
    "        embedded = embedded.permute(1, 0, 2)  # [batch_size, seq_len, embedding_dim], 同transpose\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1))  # [batch_size, 1, embedding_dim]\n",
    "        pooled = pooled.squeeze()  # [batch_size, embedding_dim]\n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_SIZE = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordAVGModel(vocab_size=VOCAB_SIZE,\n",
    "                     embedding_dim=EMBEDDING_SIZE,\n",
    "                     output_dim=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAVGModel(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8659, -0.0285, -0.5158,  ...,  0.4541, -0.2438, -1.0133],\n",
       "        [-0.3128,  0.0196,  2.1079,  ...,  0.7128, -0.3003, -1.2175],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.5654,  0.7512, -1.3183,  ..., -0.6322, -0.7870,  0.5531],\n",
       "        [ 1.1017,  0.3210, -0.5054,  ..., -0.7363, -0.6928, -0.2557],\n",
       "        [-0.0632,  0.3695,  0.3843,  ..., -1.5785, -0.0872, -0.1061]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)  # GloVe词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "crit = crit.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.train()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds, batch.label)\n",
    "        acc = binary_accuracy(preds=preds, y=batch.label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.eval()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds, batch.label)\n",
    "        acc = binary_accuracy(preds=preds, y=batch.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    model.train()\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.6858755663735526, Train Acc: 0.6244000000681196\n",
      "Epoch: 0, Valid Loss:, 0.6264575871785482, Valid Acc: 0.7109333333969117\n",
      "==============================================================================\n",
      "Epoch: 1, Train Loss: 0.6458370156424386, Train Acc: 0.7385714286395482\n",
      "Epoch: 1, Valid Loss:, 0.5106585666974386, Valid Acc: 0.7604000000635783\n",
      "==============================================================================\n",
      "Epoch: 2, Train Loss: 0.5761070107596261, Train Acc: 0.788971428666796\n",
      "Epoch: 2, Valid Loss:, 0.4487710873444875, Valid Acc: 0.7985333333651224\n",
      "==============================================================================\n",
      "Epoch: 3, Train Loss: 0.504969163261141, Train Acc: 0.8265142857415335\n",
      "Epoch: 3, Valid Loss:, 0.4276981432994207, Valid Acc: 0.8270666666984559\n",
      "==============================================================================\n",
      "Epoch: 4, Train Loss: 0.4404541014398847, Train Acc: 0.8566285714694432\n",
      "Epoch: 4, Valid Loss:, 0.4271032152891159, Valid Acc: 0.8442666666984558\n",
      "==============================================================================\n",
      "Epoch: 5, Train Loss: 0.3914074293068477, Train Acc: 0.8769714286259243\n",
      "Epoch: 5, Valid Loss:, 0.43147724928855896, Valid Acc: 0.8557333333651225\n",
      "==============================================================================\n",
      "Epoch: 6, Train Loss: 0.3509202476705824, Train Acc: 0.8907428571837289\n",
      "Epoch: 6, Valid Loss:, 0.4486611499150594, Valid Acc: 0.8646666666984558\n",
      "==============================================================================\n",
      "Epoch: 7, Train Loss: 0.3173141938209534, Train Acc: 0.9016000001089913\n",
      "Epoch: 7, Valid Loss:, 0.4694715948502223, Valid Acc: 0.8720000000317891\n",
      "==============================================================================\n",
      "Epoch: 8, Train Loss: 0.2935428521156311, Train Acc: 0.9092000000272479\n",
      "Epoch: 8, Valid Loss:, 0.4923246124267578, Valid Acc: 0.8773333333651224\n",
      "==============================================================================\n",
      "Epoch: 9, Train Loss: 0.2695511781658445, Train Acc: 0.9156571429116386\n",
      "Epoch: 9, Valid Loss:, 0.5166663384199143, Valid Acc: 0.8790666666984558\n",
      "==============================================================================\n",
      "Epoch: 10, Train Loss: 0.25286886380740575, Train Acc: 0.92068571434021\n",
      "Epoch: 10, Valid Loss:, 0.5382386282126109, Valid Acc: 0.8826666666984558\n",
      "==============================================================================\n",
      "Epoch: 11, Train Loss: 0.23378585737092156, Train Acc: 0.9269142857960292\n",
      "Epoch: 11, Valid Loss:, 0.5579450001716614, Valid Acc: 0.8841333333651225\n",
      "==============================================================================\n",
      "Epoch: 12, Train Loss: 0.22106763922827585, Train Acc: 0.9316000000272479\n",
      "Epoch: 12, Valid Loss:, 0.5764148085594177, Valid Acc: 0.8860000000317891\n",
      "==============================================================================\n",
      "Epoch: 13, Train Loss: 0.2062177593776158, Train Acc: 0.9362857143674578\n",
      "Epoch: 13, Valid Loss:, 0.5958694411754608, Valid Acc: 0.8866666666984558\n",
      "==============================================================================\n",
      "Epoch: 14, Train Loss: 0.19461874693121228, Train Acc: 0.9404571429116385\n",
      "Epoch: 14, Valid Loss:, 0.6132332356611888, Valid Acc: 0.8868000000317892\n",
      "==============================================================================\n",
      "Epoch: 15, Train Loss: 0.18234083909647805, Train Acc: 0.9440571428843907\n",
      "Epoch: 15, Valid Loss:, 0.6368893773237864, Valid Acc: 0.8873333333651224\n",
      "==============================================================================\n",
      "Epoch: 16, Train Loss: 0.17257934579849243, Train Acc: 0.9475428571973529\n",
      "Epoch: 16, Valid Loss:, 0.6511472636858622, Valid Acc: 0.8896000000317892\n",
      "==============================================================================\n",
      "Epoch: 17, Train Loss: 0.16167563836233956, Train Acc: 0.9516000000544956\n",
      "Epoch: 17, Valid Loss:, 0.6722517713546753, Valid Acc: 0.8893333333651224\n",
      "==============================================================================\n",
      "Epoch: 18, Train Loss: 0.1537216003554208, Train Acc: 0.9543428571973528\n",
      "Epoch: 18, Valid Loss:, 0.6894817413965861, Valid Acc: 0.8889333333651225\n",
      "==============================================================================\n",
      "Epoch: 19, Train Loss: 0.14491458441870553, Train Acc: 0.9570857142857143\n",
      "Epoch: 19, Valid Loss:, 0.7124210715770721, Valid Acc: 0.8904000000317891\n",
      "==============================================================================\n",
      "Epoch: 20, Train Loss: 0.13550171423213822, Train Acc: 0.9608\n",
      "Epoch: 20, Valid Loss:, 0.7321770162900288, Valid Acc: 0.8905333333651225\n",
      "==============================================================================\n",
      "Epoch: 21, Train Loss: 0.12909382121903556, Train Acc: 0.9637714285986764\n",
      "Epoch: 21, Valid Loss:, 0.7549219327290853, Valid Acc: 0.8902666666984558\n",
      "==============================================================================\n",
      "Epoch: 22, Train Loss: 0.12077329544169563, Train Acc: 0.9665142857142857\n",
      "Epoch: 22, Valid Loss:, 0.7706002835273743, Valid Acc: 0.8925333333651225\n",
      "==============================================================================\n",
      "Epoch: 23, Train Loss: 0.11416403838396072, Train Acc: 0.9697142857415335\n",
      "Epoch: 23, Valid Loss:, 0.7977890754381816, Valid Acc: 0.8898666666984558\n",
      "==============================================================================\n",
      "Epoch: 24, Train Loss: 0.10796289457593645, Train Acc: 0.9718857143129621\n",
      "Epoch: 24, Valid Loss:, 0.8190961873054504, Valid Acc: 0.8910666666984558\n",
      "==============================================================================\n",
      "Epoch: 25, Train Loss: 0.1016691980600357, Train Acc: 0.974742857170105\n",
      "Epoch: 25, Valid Loss:, 0.8442561403274537, Valid Acc: 0.8909333333651225\n",
      "==============================================================================\n",
      "Epoch: 26, Train Loss: 0.09703948827300753, Train Acc: 0.9764000000272478\n",
      "Epoch: 26, Valid Loss:, 0.8682429477850596, Valid Acc: 0.8909333333651225\n",
      "==============================================================================\n",
      "Epoch: 27, Train Loss: 0.08979941659143993, Train Acc: 0.9781714285986765\n",
      "Epoch: 27, Valid Loss:, 0.8817813575108846, Valid Acc: 0.8930666666984558\n",
      "==============================================================================\n",
      "Epoch: 28, Train Loss: 0.08536963745866503, Train Acc: 0.9802285714558193\n",
      "Epoch: 28, Valid Loss:, 0.9112996077378591, Valid Acc: 0.8909333333651225\n",
      "==============================================================================\n",
      "Epoch: 29, Train Loss: 0.08023812718561718, Train Acc: 0.9816\n",
      "Epoch: 29, Valid Loss:, 0.929311740620931, Valid Acc: 0.8922666666984558\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, crit)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, crit)\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(), 'word_avg_model.pth')\n",
    "        \n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Train Acc: {train_acc}')\n",
    "    print(f'Epoch: {epoch}, Valid Loss:, {valid_loss}, Valid Acc: {valid_acc}')\n",
    "    print('======' * 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./word_avg_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sentence):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(DEVICE)  # seq_len\n",
    "    tensor = tensor.unsqueeze(1)  # seq_len * batch_size=1\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998784065246582"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment('I like this food so much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.477886123931967e-06"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment('I hate this food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))  # [sent_len, batch_size, emd_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        # output = [sent_len, batch_size, hid_dim * num_directions]\n",
    "        # hidden = [num_layers * num_directions, batch_size, hid_dim]\n",
    "        # cell = [num_layers * num_directions, batch_size, hid_dim]\n",
    "        \n",
    "        # concat the final forward (hidden[-2, :, :]) and backward (hidden[-1, :, :]) hidden layers and apply dropout\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))  # [batch_size, hid_dim * num_directions]\n",
    "        return self.linear(hidden.squeeze(0))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=INPUT_DIM,\n",
    "                 embedding_dim=EMBEDDING_DIM,\n",
    "                 hidden_dim=HIDDEN_DIM,\n",
    "                 output_dim=OUTPUT_DIM,\n",
    "                 n_layers=N_LAYERS,\n",
    "                 bidirectional=BIDIRECTIONAL,\n",
    "                 dropout=DROPOUT,\n",
    "                 pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4810857 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "print(f'The model has {count_parameters(model)} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "print((model.embedding.weight.data).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.5654,  0.7512, -1.3183,  ..., -0.6322, -0.7870,  0.5531],\n",
       "        [ 1.1017,  0.3210, -0.5054,  ..., -0.7363, -0.6928, -0.2557],\n",
       "        [-0.0632,  0.3695,  0.3843,  ..., -1.5785, -0.0872, -0.1061]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.694 | Train Acc: 50.80%\n",
      "\t Valid Loss: 0.693 | Valid Acc: 51.37%\n",
      "Epoch: 02 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.693 | Train Acc: 51.09%\n",
      "\t Valid Loss: 0.693 | Valid Acc: 50.63%\n",
      "Epoch: 03 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.689 | Train Acc: 53.52%\n",
      "\t Valid Loss: 0.672 | Valid Acc: 59.53%\n",
      "Epoch: 04 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.667 | Train Acc: 59.52%\n",
      "\t Valid Loss: 0.668 | Valid Acc: 61.39%\n",
      "Epoch: 05 & Epoch Time: 1m 30s\n",
      "\t Train Loss: 0.542 | Train Acc: 73.83%\n",
      "\t Valid Loss: 0.356 | Valid Acc: 85.53%\n",
      "Epoch: 06 & Epoch Time: 1m 30s\n",
      "\t Train Loss: 0.327 | Train Acc: 86.86%\n",
      "\t Valid Loss: 0.324 | Valid Acc: 87.83%\n",
      "Epoch: 07 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.258 | Train Acc: 90.18%\n",
      "\t Valid Loss: 0.285 | Valid Acc: 88.75%\n",
      "Epoch: 08 & Epoch Time: 1m 29s\n",
      "\t Train Loss: 0.209 | Train Acc: 92.07%\n",
      "\t Valid Loss: 0.301 | Valid Acc: 88.72%\n",
      "Epoch: 09 & Epoch Time: 1m 30s\n",
      "\t Train Loss: 0.179 | Train Acc: 93.58%\n",
      "\t Valid Loss: 0.301 | Valid Acc: 89.43%\n",
      "Epoch: 10 & Epoch Time: 1m 30s\n",
      "\t Train Loss: 0.150 | Train Acc: 94.45%\n",
      "\t Valid Loss: 0.268 | Valid Acc: 89.64%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model,\n",
    "                                  iterator=train_iter,\n",
    "                                  optimizer=optimizer,\n",
    "                                  crit=crit)\n",
    "    valid_loss, valid_acc = evaluate(model, iterator=valid_iter, crit=crit)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} & Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we'll improve in the next notebook.\n",
    "\n",
    "- Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.285 | Test Acc: 88.81%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_lstm_model.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_iter, crit)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, output_size, pad_idx, num_filters, filter_size, dropout):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embedding_size))\n",
    "        self.linear = nn.Linear(embedding_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)  # [batch_size, seq_len]\n",
    "        embedded = self.embed(text)  # [batch_size, seq_len, embedding_size]\n",
    "        embedded = embedded.unsqueeze(1)  # [batch_size, 1, seq_len, embedding_size]\n",
    "        conved = F.relu(self.conv(embedded))  # [batch_size, num_filters, seq_len-filter_size+1, 1]\n",
    "        conved = conved.squeeze(3)  # [batch_size, num_filters, seq_len-filter_size+1]\n",
    "        pooled = F.max_pool1d(conved, conved.shape[2])  # [batch_size, num_filters, 1]\n",
    "        pooled = pooled.squeeze(2)  # [batch_size, num_filters]\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.linear(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(vocab_size=VOCAB_SIZE,\n",
    "                embedding_size=EMBEDDING_SIZE,\n",
    "                output_size=OUTPUT_SIZE,\n",
    "                pad_idx=PAD_IDX,\n",
    "                num_filters=100,\n",
    "                filter_size=3,\n",
    "                dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (embed): Embedding(25002, 100, padding_idx=1)\n",
       "  (conv): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)  # GloVe词向量\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "crit = crit.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.train()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds, batch.label)\n",
    "        acc = binary_accuracy(preds=preds, y=batch.label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, crit):\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    model.eval()\n",
    "    total_len = 0.\n",
    "    for batch in iterator:\n",
    "        preds = model(batch.text).squeeze()\n",
    "        loss = crit(preds, batch.label)\n",
    "        acc = binary_accuracy(preds=preds, y=batch.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "    model.train()\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 & Epoch Time: 0m 4s\n",
      "\t Train Loss: 0.685 | Train Acc: 61.43%\n",
      "\t Valid Loss: 0.624 | Valid Acc: 70.04%\n",
      "Epoch: 02 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.644 | Train Acc: 74.64%\n",
      "\t Valid Loss: 0.500 | Valid Acc: 76.40%\n",
      "Epoch: 03 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.572 | Train Acc: 79.32%\n",
      "\t Valid Loss: 0.453 | Valid Acc: 79.81%\n",
      "Epoch: 04 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.497 | Train Acc: 82.94%\n",
      "\t Valid Loss: 0.428 | Valid Acc: 82.96%\n",
      "Epoch: 05 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.435 | Train Acc: 86.01%\n",
      "\t Valid Loss: 0.428 | Valid Acc: 84.60%\n",
      "Epoch: 06 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.385 | Train Acc: 87.80%\n",
      "\t Valid Loss: 0.445 | Valid Acc: 85.67%\n",
      "Epoch: 07 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.347 | Train Acc: 89.11%\n",
      "\t Valid Loss: 0.458 | Valid Acc: 86.48%\n",
      "Epoch: 08 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.317 | Train Acc: 90.19%\n",
      "\t Valid Loss: 0.482 | Valid Acc: 87.00%\n",
      "Epoch: 09 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.290 | Train Acc: 90.89%\n",
      "\t Valid Loss: 0.502 | Valid Acc: 87.52%\n",
      "Epoch: 10 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.270 | Train Acc: 91.67%\n",
      "\t Valid Loss: 0.522 | Valid Acc: 87.95%\n",
      "Epoch: 11 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.249 | Train Acc: 92.16%\n",
      "\t Valid Loss: 0.542 | Valid Acc: 88.20%\n",
      "Epoch: 12 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.234 | Train Acc: 92.71%\n",
      "\t Valid Loss: 0.564 | Valid Acc: 88.25%\n",
      "Epoch: 13 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.221 | Train Acc: 93.19%\n",
      "\t Valid Loss: 0.586 | Valid Acc: 88.40%\n",
      "Epoch: 14 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.207 | Train Acc: 93.72%\n",
      "\t Valid Loss: 0.602 | Valid Acc: 88.60%\n",
      "Epoch: 15 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.193 | Train Acc: 94.03%\n",
      "\t Valid Loss: 0.624 | Valid Acc: 88.65%\n",
      "Epoch: 16 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.182 | Train Acc: 94.43%\n",
      "\t Valid Loss: 0.646 | Valid Acc: 88.69%\n",
      "Epoch: 17 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.172 | Train Acc: 94.83%\n",
      "\t Valid Loss: 0.666 | Valid Acc: 88.79%\n",
      "Epoch: 18 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.162 | Train Acc: 95.17%\n",
      "\t Valid Loss: 0.678 | Valid Acc: 88.88%\n",
      "Epoch: 19 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.152 | Train Acc: 95.51%\n",
      "\t Valid Loss: 0.702 | Valid Acc: 88.92%\n",
      "Epoch: 20 & Epoch Time: 0m 3s\n",
      "\t Train Loss: 0.145 | Train Acc: 95.82%\n",
      "\t Valid Loss: 0.724 | Valid Acc: 88.77%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, iterator=train_iter, optimizer=optimizer, crit=crit)\n",
    "    valid_loss, valid_acc = evaluate(model, iterator=valid_iter, crit=crit)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    " \n",
    "    print(f'Epoch: {epoch+1:02} & Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.435 | Test Acc: 84.20%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_cnn_model.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_iter, crit)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModelList(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, pad_idx):\n",
    "        super(CNNModelList, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.linear = nn.Linear(in_features=len(filter_sizes) * n_filters, out_features=output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0)\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]  # 对维度2进行max_pool\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.linear(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 100\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModelList(vocab_size=VOCAB_SIZE, \n",
    "                     embedding_dim=EMBEDDING_DIM, \n",
    "                     n_filters=N_FILTERS, \n",
    "                     filter_sizes=FILTER_SIZES, \n",
    "                     output_dim=OUTPUT_DIM, \n",
    "                     dropout=DROPOUT, \n",
    "                     pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.653 | Train Acc: 61.37%\n",
      "\t Valid Loss: 0.510 | Valid Acc: 77.84%\n",
      "Epoch: 02 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.419 | Train Acc: 81.10%\n",
      "\t Valid Loss: 0.352 | Valid Acc: 85.11%\n",
      "Epoch: 03 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.293 | Train Acc: 87.73%\n",
      "\t Valid Loss: 0.318 | Valid Acc: 86.56%\n",
      "Epoch: 04 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.207 | Train Acc: 91.82%\n",
      "\t Valid Loss: 0.316 | Valid Acc: 87.12%\n",
      "Epoch: 05 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.149 | Train Acc: 94.49%\n",
      "\t Valid Loss: 0.340 | Valid Acc: 86.76%\n",
      "Epoch: 06 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.099 | Train Acc: 96.74%\n",
      "\t Valid Loss: 0.359 | Valid Acc: 86.89%\n",
      "Epoch: 07 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.068 | Train Acc: 97.98%\n",
      "\t Valid Loss: 0.394 | Valid Acc: 86.88%\n",
      "Epoch: 08 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.050 | Train Acc: 98.43%\n",
      "\t Valid Loss: 0.408 | Valid Acc: 87.04%\n",
      "Epoch: 09 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.040 | Train Acc: 98.80%\n",
      "\t Valid Loss: 0.443 | Valid Acc: 86.84%\n",
      "Epoch: 10 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.026 | Train Acc: 99.29%\n",
      "\t Valid Loss: 0.498 | Valid Acc: 86.64%\n",
      "Epoch: 11 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.020 | Train Acc: 99.49%\n",
      "\t Valid Loss: 0.511 | Valid Acc: 86.88%\n",
      "Epoch: 12 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.017 | Train Acc: 99.50%\n",
      "\t Valid Loss: 0.572 | Valid Acc: 86.44%\n",
      "Epoch: 13 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.015 | Train Acc: 99.59%\n",
      "\t Valid Loss: 0.578 | Valid Acc: 86.81%\n",
      "Epoch: 14 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.014 | Train Acc: 99.63%\n",
      "\t Valid Loss: 0.604 | Valid Acc: 86.53%\n",
      "Epoch: 15 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.013 | Train Acc: 99.70%\n",
      "\t Valid Loss: 0.623 | Valid Acc: 86.60%\n",
      "Epoch: 16 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.010 | Train Acc: 99.71%\n",
      "\t Valid Loss: 0.660 | Valid Acc: 86.57%\n",
      "Epoch: 17 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.008 | Train Acc: 99.75%\n",
      "\t Valid Loss: 0.686 | Valid Acc: 86.84%\n",
      "Epoch: 18 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.008 | Train Acc: 99.80%\n",
      "\t Valid Loss: 0.725 | Valid Acc: 86.41%\n",
      "Epoch: 19 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.007 | Train Acc: 99.85%\n",
      "\t Valid Loss: 0.746 | Valid Acc: 86.77%\n",
      "Epoch: 20 & Epoch Time: 0m 13s\n",
      "\t Train Loss: 0.009 | Train Acc: 99.70%\n",
      "\t Valid Loss: 0.773 | Valid Acc: 86.71%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, iterator=train_iter, optimizer=optimizer, crit=crit)\n",
    "    valid_loss, valid_acc = evaluate(model, iterator=valid_iter, crit=crit)\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_cnn_model_list.pth')\n",
    " \n",
    "    print(f'Epoch: {epoch+1:02} & Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.337 ! Test Acc: 85.81%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./best_cnn_model_list.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_iter, crit)\n",
    "print(f'Test Loss: {test_loss:.3f} ! Test Acc: {test_acc * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Torch10] *",
   "language": "python",
   "name": "conda-env-Torch10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
