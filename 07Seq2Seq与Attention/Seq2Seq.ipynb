{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jieba\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from collections import Counter\n",
    "from langconv import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('C:\\\\Users\\\\Administrator\\\\nltk_data')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.data.find('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入中英文数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file):\n",
    "    en = []\n",
    "    cn = []\n",
    "    num_examples = 0\n",
    "    with open(file=in_file, mode='r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            line[1] = Converter('zh-hans').convert(line[1])\n",
    "            en.append(['BOS'] + nltk.word_tokenize(line[0].lower()) + ['EOS'])\n",
    "            cn.append(['BOS'] + list(jieba.cut(line[1])) + ['EOS'])\n",
    "    return en, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.552 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_file = './nmt/en-cn/train.txt'\n",
    "dev_file = './nmt/en-cn/dev.txt'\n",
    "train_en, train_cn = load_data(in_file=train_file)\n",
    "dev_en, dev_cn = load_data(in_file=dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14533 14533\n"
     ]
    }
   ],
   "source": [
    "print(len(train_en), len(train_cn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'], ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS'], ['BOS', 'she', 'married', 'him', '.', 'EOS'], ['BOS', 'i', 'do', \"n't\", 'like', 'learning', 'irregular', 'verbs', '.', 'EOS'], ['BOS', 'it', \"'s\", 'a', 'whole', 'new', 'ball', 'game', 'for', 'me', '.', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_en[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BOS', '任何人', '都', '可以', '做到', '。', 'EOS'], ['BOS', '要', '不要', '再', '来', '一块', '蛋糕', '？', 'EOS'], ['BOS', '她', '嫁给', '了', '他', '。', 'EOS'], ['BOS', '我', '不', '喜欢', '学习', '不规则', '动词', '。', 'EOS'], ['BOS', '这', '对', '我', '来说', '是', '个', '全新', '的', '球类', '游戏', '。', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(train_cn[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建单词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0\n",
    "PAD_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for sent in sentence:\n",
    "            word_count[sent] += 1\n",
    "    ls = word_count.most_common(max_words)\n",
    "    total_words = len(ls) + 2\n",
    "    word_dict = {word[0]: index + 2 for index, word in enumerate(ls)}\n",
    "    word_dict['UNK'] = UNK_IDX\n",
    "    word_dict['PAD'] = PAD_IDX\n",
    "    return word_dict, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict, en_total_words = build_dict(train_en)\n",
    "cn_dict, cn_total_words = build_dict(train_cn)\n",
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(cn_dict)\n",
    "# print(cn_total_words)\n",
    "# print(inv_en_dict)\n",
    "# print(inv_cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将单词全部转变为数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    return out_en_sentences, out_cn_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS for what purpose did he come here ? EOS\n",
      "BOS 他来 这里 的 目的 是 什么 ？ EOS\n"
     ]
    }
   ],
   "source": [
    "k = 10000\n",
    "print(' '.join([inv_en_dict[i] for i in train_en[k]]))\n",
    "print(' '.join([inv_cn_dict[i] for i in train_cn[k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把全部句子分成batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)  # 初始batch开始索引位置\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))  # 所有batch放在一个大列表里\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]),\n",
       " array([15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(n=100, minibatch_size=15)  # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   2,   12,  167,   23,  114,    5,   27, 1755,    4,    3],\n",
       "        [   3,    9,   13,   45,   33,    9,    9,    0,    0,    0]]),\n",
       " array([10,  7]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seqs = [[2, 12, 167, 23, 114, 5, 27, 1755, 4, 3], [3, 9, 13, 45, 33, 9, 9]]\n",
    "prepare_data(seqs=test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(n=len(en_sentences), minibatch_size=batch_size)\n",
    "    all_ex = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)  # mb_cn_sentences -> mb_en_sentences\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))  # 一个列表为一个batch数据，所有batch组成一个大列表数据\n",
    "    return all_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = gen_examples(en_sentences=train_en, cn_sentences=train_cn, batch_size=batch_size)\n",
    "random.shuffle(train_data)\n",
    "dev_data = gen_examples(en_sentences=dev_en, cn_sentences=dev_cn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):  # lengths表示batch里每个句子的长度\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        packed_embedded = pack_padded_sequence(\n",
    "            input=embedded,\n",
    "            lengths=sorted_len.long().cpu().data.numpy(),\n",
    "            batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(sequence=packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        return out, hid[[-1]]  # 有时候num_layers层数多，需要取出最后一层        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(PlainDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]  # 隐藏层也要排序\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        packed_seq = pack_padded_sequence(\n",
    "            input=y_sorted,\n",
    "            lengths=sorted_len.long().cpu().data.numpy(),\n",
    "            batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        output = F.log_softmax(self.out(output_seq), -1)\n",
    "        return output, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(PlainSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)  # 调用PlainEncoder里面forward的方法，返回out和hid\n",
    "        output, hid = self.decoder(y=y, y_lengths=y_lengths, hid=hid)  # 调用PlainDecoder里面forward的方法\n",
    "        return output, None\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=10):  # 这里的y是BOS的数值索引，为2\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y=y, y_lengths=torch.ones(batch_size).long().to(y.device), hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "    \n",
    "    def forward(self, input, target, mask):\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask  # gather(dim=1)对Tensor每行进行列索引\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传入中文和英文参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PlainEncoder(vocab_size=en_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = PlainDecoder(vocab_size=cn_total_words, hidden_size=hidden_size, dropout=dropout)\n",
    "model = PlainSeq2Seq(encoder=encoder, decoder=decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len - 1).to(device).long()\n",
    "            mb_y_len[mb_y_len <= 0] = 1\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "        print(f'Evaluation loss: {total_loss / total_num_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0.\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len - 1).to(device).long()\n",
    "            mb_y_len[mb_y_len <= 0] = 1\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]  # padding的位置设置为0，其他位置设置为1;max(), 记得加括号\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)  # 梯度裁剪\n",
    "            optimizer.step()\n",
    "            if it % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Iteration: {it}, Loss: {loss.item()}')\n",
    "        print(f'Epoch: {epoch}, Training loss: {total_loss / total_num_words}')\n",
    "        if epoch % 10 == 0:\n",
    "            evaluate(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 9.162109375\n",
      "Epoch: 0, Iteration: 100, Loss: 5.5519208908081055\n",
      "Epoch: 0, Iteration: 200, Loss: 5.0611348152160645\n",
      "Epoch: 0, Training loss: 5.624579241444214\n",
      "Evaluation loss: 4.950168485393913\n",
      "Epoch: 1, Iteration: 0, Loss: 4.796090126037598\n",
      "Epoch: 1, Iteration: 100, Loss: 5.11106014251709\n",
      "Epoch: 1, Iteration: 200, Loss: 4.652934551239014\n",
      "Epoch: 1, Training loss: 4.672120515636809\n",
      "Epoch: 2, Iteration: 0, Loss: 4.415272235870361\n",
      "Epoch: 2, Iteration: 100, Loss: 4.7995829582214355\n",
      "Epoch: 2, Iteration: 200, Loss: 4.372103214263916\n",
      "Epoch: 2, Training loss: 4.3419556214933035\n",
      "Epoch: 3, Iteration: 0, Loss: 4.1346001625061035\n",
      "Epoch: 3, Iteration: 100, Loss: 4.575839042663574\n",
      "Epoch: 3, Iteration: 200, Loss: 4.130451202392578\n",
      "Epoch: 3, Training loss: 4.10210233931212\n",
      "Epoch: 4, Iteration: 0, Loss: 3.9095547199249268\n",
      "Epoch: 4, Iteration: 100, Loss: 4.386038303375244\n",
      "Epoch: 4, Iteration: 200, Loss: 3.888876438140869\n",
      "Epoch: 4, Training loss: 3.9019035238671433\n",
      "Epoch: 5, Iteration: 0, Loss: 3.698808193206787\n",
      "Epoch: 5, Iteration: 100, Loss: 4.228642463684082\n",
      "Epoch: 5, Iteration: 200, Loss: 3.725515842437744\n",
      "Epoch: 5, Training loss: 3.728543375959142\n",
      "Epoch: 6, Iteration: 0, Loss: 3.5041074752807617\n",
      "Epoch: 6, Iteration: 100, Loss: 4.047641754150391\n",
      "Epoch: 6, Iteration: 200, Loss: 3.5722646713256836\n",
      "Epoch: 6, Training loss: 3.5703504026714\n",
      "Epoch: 7, Iteration: 0, Loss: 3.3584229946136475\n",
      "Epoch: 7, Iteration: 100, Loss: 3.9300968647003174\n",
      "Epoch: 7, Iteration: 200, Loss: 3.443135976791382\n",
      "Epoch: 7, Training loss: 3.4301323620776216\n",
      "Epoch: 8, Iteration: 0, Loss: 3.1907100677490234\n",
      "Epoch: 8, Iteration: 100, Loss: 3.814054250717163\n",
      "Epoch: 8, Iteration: 200, Loss: 3.30499529838562\n",
      "Epoch: 8, Training loss: 3.3008910679117114\n",
      "Epoch: 9, Iteration: 0, Loss: 3.063995838165283\n",
      "Epoch: 9, Iteration: 100, Loss: 3.6892247200012207\n",
      "Epoch: 9, Iteration: 200, Loss: 3.205702304840088\n",
      "Epoch: 9, Training loss: 3.1817975354406864\n",
      "Epoch: 10, Iteration: 0, Loss: 2.9230875968933105\n",
      "Epoch: 10, Iteration: 100, Loss: 3.5520286560058594\n",
      "Epoch: 10, Iteration: 200, Loss: 3.0782110691070557\n",
      "Epoch: 10, Training loss: 3.072055585205204\n",
      "Evaluation loss: 3.98651900345513\n",
      "Epoch: 11, Iteration: 0, Loss: 2.803680658340454\n",
      "Epoch: 11, Iteration: 100, Loss: 3.4690680503845215\n",
      "Epoch: 11, Iteration: 200, Loss: 2.9406204223632812\n",
      "Epoch: 11, Training loss: 2.971170659171655\n",
      "Epoch: 12, Iteration: 0, Loss: 2.6926803588867188\n",
      "Epoch: 12, Iteration: 100, Loss: 3.3748838901519775\n",
      "Epoch: 12, Iteration: 200, Loss: 2.8827602863311768\n",
      "Epoch: 12, Training loss: 2.8764304386936557\n",
      "Epoch: 13, Iteration: 0, Loss: 2.5815939903259277\n",
      "Epoch: 13, Iteration: 100, Loss: 3.2788119316101074\n",
      "Epoch: 13, Iteration: 200, Loss: 2.798469305038452\n",
      "Epoch: 13, Training loss: 2.786544450935307\n",
      "Epoch: 14, Iteration: 0, Loss: 2.471810817718506\n",
      "Epoch: 14, Iteration: 100, Loss: 3.1623079776763916\n",
      "Epoch: 14, Iteration: 200, Loss: 2.692845106124878\n",
      "Epoch: 14, Training loss: 2.7013660186854263\n",
      "Epoch: 15, Iteration: 0, Loss: 2.363186836242676\n",
      "Epoch: 15, Iteration: 100, Loss: 3.1118147373199463\n",
      "Epoch: 15, Iteration: 200, Loss: 2.5878264904022217\n",
      "Epoch: 15, Training loss: 2.6237222706997207\n",
      "Epoch: 16, Iteration: 0, Loss: 2.2979447841644287\n",
      "Epoch: 16, Iteration: 100, Loss: 3.0516886711120605\n",
      "Epoch: 16, Iteration: 200, Loss: 2.520172357559204\n",
      "Epoch: 16, Training loss: 2.5472229258302965\n",
      "Epoch: 17, Iteration: 0, Loss: 2.2269439697265625\n",
      "Epoch: 17, Iteration: 100, Loss: 2.938235282897949\n",
      "Epoch: 17, Iteration: 200, Loss: 2.442086935043335\n",
      "Epoch: 17, Training loss: 2.4782993435105056\n",
      "Epoch: 18, Iteration: 0, Loss: 2.151527166366577\n",
      "Epoch: 18, Iteration: 100, Loss: 2.906468152999878\n",
      "Epoch: 18, Iteration: 200, Loss: 2.337827205657959\n",
      "Epoch: 18, Training loss: 2.411563710981552\n",
      "Epoch: 19, Iteration: 0, Loss: 2.070251703262329\n",
      "Epoch: 19, Iteration: 100, Loss: 2.831300735473633\n",
      "Epoch: 19, Iteration: 200, Loss: 2.284564971923828\n",
      "Epoch: 19, Training loss: 2.3496492970688596\n",
      "Epoch: 20, Iteration: 0, Loss: 1.992849588394165\n",
      "Epoch: 20, Iteration: 100, Loss: 2.7775068283081055\n",
      "Epoch: 20, Iteration: 200, Loss: 2.249311685562134\n",
      "Epoch: 20, Training loss: 2.2905631461154656\n",
      "Evaluation loss: 3.9553991844201457\n",
      "Epoch: 21, Iteration: 0, Loss: 1.9612743854522705\n",
      "Epoch: 21, Iteration: 100, Loss: 2.688706159591675\n",
      "Epoch: 21, Iteration: 200, Loss: 2.1752877235412598\n",
      "Epoch: 21, Training loss: 2.2354325380859854\n",
      "Epoch: 22, Iteration: 0, Loss: 1.8962366580963135\n",
      "Epoch: 22, Iteration: 100, Loss: 2.59677791595459\n",
      "Epoch: 22, Iteration: 200, Loss: 2.1786422729492188\n",
      "Epoch: 22, Training loss: 2.1794103304845946\n",
      "Epoch: 23, Iteration: 0, Loss: 1.817850947380066\n",
      "Epoch: 23, Iteration: 100, Loss: 2.574559211730957\n",
      "Epoch: 23, Iteration: 200, Loss: 2.1025874614715576\n",
      "Epoch: 23, Training loss: 2.128355990073023\n",
      "Epoch: 24, Iteration: 0, Loss: 1.7566159963607788\n",
      "Epoch: 24, Iteration: 100, Loss: 2.5058751106262207\n",
      "Epoch: 24, Iteration: 200, Loss: 2.0367984771728516\n",
      "Epoch: 24, Training loss: 2.0811239255008163\n",
      "Epoch: 25, Iteration: 0, Loss: 1.7118483781814575\n",
      "Epoch: 25, Iteration: 100, Loss: 2.4883124828338623\n",
      "Epoch: 25, Iteration: 200, Loss: 1.9919248819351196\n",
      "Epoch: 25, Training loss: 2.037603221635538\n",
      "Epoch: 26, Iteration: 0, Loss: 1.6667653322219849\n",
      "Epoch: 26, Iteration: 100, Loss: 2.4118340015411377\n",
      "Epoch: 26, Iteration: 200, Loss: 1.9357573986053467\n",
      "Epoch: 26, Training loss: 1.9957048538915063\n",
      "Epoch: 27, Iteration: 0, Loss: 1.61398184299469\n",
      "Epoch: 27, Iteration: 100, Loss: 2.4110987186431885\n",
      "Epoch: 27, Iteration: 200, Loss: 1.8951947689056396\n",
      "Epoch: 27, Training loss: 1.9529709296240367\n",
      "Epoch: 28, Iteration: 0, Loss: 1.561326026916504\n",
      "Epoch: 28, Iteration: 100, Loss: 2.386634588241577\n",
      "Epoch: 28, Iteration: 200, Loss: 1.843527913093567\n",
      "Epoch: 28, Training loss: 1.9105436077736584\n",
      "Epoch: 29, Iteration: 0, Loss: 1.555423617362976\n",
      "Epoch: 29, Iteration: 100, Loss: 2.3238303661346436\n",
      "Epoch: 29, Iteration: 200, Loss: 1.835247278213501\n",
      "Epoch: 29, Training loss: 1.877628411506624\n",
      "Epoch: 30, Iteration: 0, Loss: 1.5220065116882324\n",
      "Epoch: 30, Iteration: 100, Loss: 2.2934982776641846\n",
      "Epoch: 30, Iteration: 200, Loss: 1.737573504447937\n",
      "Epoch: 30, Training loss: 1.8378872894766871\n",
      "Evaluation loss: 3.9812512625608716\n",
      "Epoch: 31, Iteration: 0, Loss: 1.4654498100280762\n",
      "Epoch: 31, Iteration: 100, Loss: 2.2290923595428467\n",
      "Epoch: 31, Iteration: 200, Loss: 1.767273187637329\n",
      "Epoch: 31, Training loss: 1.8038638903480597\n",
      "Epoch: 32, Iteration: 0, Loss: 1.453244686126709\n",
      "Epoch: 32, Iteration: 100, Loss: 2.193340301513672\n",
      "Epoch: 32, Iteration: 200, Loss: 1.7004321813583374\n",
      "Epoch: 32, Training loss: 1.7720015260771882\n",
      "Epoch: 33, Iteration: 0, Loss: 1.409743309020996\n",
      "Epoch: 33, Iteration: 100, Loss: 2.143735885620117\n",
      "Epoch: 33, Iteration: 200, Loss: 1.685673713684082\n",
      "Epoch: 33, Training loss: 1.7397570973391\n",
      "Epoch: 34, Iteration: 0, Loss: 1.3715019226074219\n",
      "Epoch: 34, Iteration: 100, Loss: 2.151315689086914\n",
      "Epoch: 34, Iteration: 200, Loss: 1.6657140254974365\n",
      "Epoch: 34, Training loss: 1.706604306941798\n",
      "Epoch: 35, Iteration: 0, Loss: 1.3632301092147827\n",
      "Epoch: 35, Iteration: 100, Loss: 2.0930557250976562\n",
      "Epoch: 35, Iteration: 200, Loss: 1.652679204940796\n",
      "Epoch: 35, Training loss: 1.67548606786075\n",
      "Epoch: 36, Iteration: 0, Loss: 1.343354344367981\n",
      "Epoch: 36, Iteration: 100, Loss: 2.074387550354004\n",
      "Epoch: 36, Iteration: 200, Loss: 1.5769051313400269\n",
      "Epoch: 36, Training loss: 1.649548152818482\n",
      "Epoch: 37, Iteration: 0, Loss: 1.2998743057250977\n",
      "Epoch: 37, Iteration: 100, Loss: 2.029456615447998\n",
      "Epoch: 37, Iteration: 200, Loss: 1.5485308170318604\n",
      "Epoch: 37, Training loss: 1.6162367337695445\n",
      "Epoch: 38, Iteration: 0, Loss: 1.263327956199646\n",
      "Epoch: 38, Iteration: 100, Loss: 2.023247718811035\n",
      "Epoch: 38, Iteration: 200, Loss: 1.5380053520202637\n",
      "Epoch: 38, Training loss: 1.5955727969034232\n",
      "Epoch: 39, Iteration: 0, Loss: 1.2563713788986206\n",
      "Epoch: 39, Iteration: 100, Loss: 1.926939845085144\n",
      "Epoch: 39, Iteration: 200, Loss: 1.5011212825775146\n",
      "Epoch: 39, Training loss: 1.5649081057650847\n",
      "Epoch: 40, Iteration: 0, Loss: 1.234930157661438\n",
      "Epoch: 40, Iteration: 100, Loss: 1.934259295463562\n",
      "Epoch: 40, Iteration: 200, Loss: 1.4894301891326904\n",
      "Epoch: 40, Training loss: 1.5402339772330598\n",
      "Evaluation loss: 4.063679401773543\n",
      "Epoch: 41, Iteration: 0, Loss: 1.1923120021820068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Iteration: 100, Loss: 1.9102275371551514\n",
      "Epoch: 41, Iteration: 200, Loss: 1.4650654792785645\n",
      "Epoch: 41, Training loss: 1.5175493052178421\n",
      "Epoch: 42, Iteration: 0, Loss: 1.2017388343811035\n",
      "Epoch: 42, Iteration: 100, Loss: 1.8336445093154907\n",
      "Epoch: 42, Iteration: 200, Loss: 1.4173647165298462\n",
      "Epoch: 42, Training loss: 1.4930325140211504\n",
      "Epoch: 43, Iteration: 0, Loss: 1.1226239204406738\n",
      "Epoch: 43, Iteration: 100, Loss: 1.8834588527679443\n",
      "Epoch: 43, Iteration: 200, Loss: 1.4309792518615723\n",
      "Epoch: 43, Training loss: 1.4700407962536666\n",
      "Epoch: 44, Iteration: 0, Loss: 1.1705979108810425\n",
      "Epoch: 44, Iteration: 100, Loss: 1.887696385383606\n",
      "Epoch: 44, Iteration: 200, Loss: 1.3904147148132324\n",
      "Epoch: 44, Training loss: 1.4500091486918358\n",
      "Epoch: 45, Iteration: 0, Loss: 1.089949131011963\n",
      "Epoch: 45, Iteration: 100, Loss: 1.8100453615188599\n",
      "Epoch: 45, Iteration: 200, Loss: 1.3520302772521973\n",
      "Epoch: 45, Training loss: 1.4237448310104193\n",
      "Epoch: 46, Iteration: 0, Loss: 1.0702838897705078\n",
      "Epoch: 46, Iteration: 100, Loss: 1.8012094497680664\n",
      "Epoch: 46, Iteration: 200, Loss: 1.3192590475082397\n",
      "Epoch: 46, Training loss: 1.4019895075332298\n",
      "Epoch: 47, Iteration: 0, Loss: 1.094563603401184\n",
      "Epoch: 47, Iteration: 100, Loss: 1.8041691780090332\n",
      "Epoch: 47, Iteration: 200, Loss: 1.293674349784851\n",
      "Epoch: 47, Training loss: 1.3871120665296652\n",
      "Epoch: 48, Iteration: 0, Loss: 1.058559775352478\n",
      "Epoch: 48, Iteration: 100, Loss: 1.7609652280807495\n",
      "Epoch: 48, Iteration: 200, Loss: 1.2817267179489136\n",
      "Epoch: 48, Training loss: 1.3650266613563677\n",
      "Epoch: 49, Iteration: 0, Loss: 1.0358136892318726\n",
      "Epoch: 49, Iteration: 100, Loss: 1.7033662796020508\n",
      "Epoch: 49, Iteration: 200, Loss: 1.2841973304748535\n",
      "Epoch: 49, Training loss: 1.3464699353893537\n",
      "Epoch: 50, Iteration: 0, Loss: 1.016066074371338\n",
      "Epoch: 50, Iteration: 100, Loss: 1.717380166053772\n",
      "Epoch: 50, Iteration: 200, Loss: 1.2976936101913452\n",
      "Epoch: 50, Training loss: 1.3262375475352213\n",
      "Evaluation loss: 4.161089842932623\n",
      "Epoch: 51, Iteration: 0, Loss: 0.9870522618293762\n",
      "Epoch: 51, Iteration: 100, Loss: 1.711735725402832\n",
      "Epoch: 51, Iteration: 200, Loss: 1.2387330532073975\n",
      "Epoch: 51, Training loss: 1.3066629800752372\n",
      "Epoch: 52, Iteration: 0, Loss: 1.0087522268295288\n",
      "Epoch: 52, Iteration: 100, Loss: 1.6589784622192383\n",
      "Epoch: 52, Iteration: 200, Loss: 1.2487220764160156\n",
      "Epoch: 52, Training loss: 1.2882388714544781\n",
      "Epoch: 53, Iteration: 0, Loss: 1.0030354261398315\n",
      "Epoch: 53, Iteration: 100, Loss: 1.6512601375579834\n",
      "Epoch: 53, Iteration: 200, Loss: 1.164770245552063\n",
      "Epoch: 53, Training loss: 1.271648171718674\n",
      "Epoch: 54, Iteration: 0, Loss: 1.0093212127685547\n",
      "Epoch: 54, Iteration: 100, Loss: 1.6520944833755493\n",
      "Epoch: 54, Iteration: 200, Loss: 1.2035831212997437\n",
      "Epoch: 54, Training loss: 1.256641396760298\n",
      "Epoch: 55, Iteration: 0, Loss: 0.9238237142562866\n",
      "Epoch: 55, Iteration: 100, Loss: 1.6970373392105103\n",
      "Epoch: 55, Iteration: 200, Loss: 1.1198302507400513\n",
      "Epoch: 55, Training loss: 1.2376956194480673\n",
      "Epoch: 56, Iteration: 0, Loss: 0.9173389673233032\n",
      "Epoch: 56, Iteration: 100, Loss: 1.5626413822174072\n",
      "Epoch: 56, Iteration: 200, Loss: 1.131614327430725\n",
      "Epoch: 56, Training loss: 1.2219314346916583\n",
      "Epoch: 57, Iteration: 0, Loss: 0.9141749739646912\n",
      "Epoch: 57, Iteration: 100, Loss: 1.6019243001937866\n",
      "Epoch: 57, Iteration: 200, Loss: 1.1132038831710815\n",
      "Epoch: 57, Training loss: 1.209861110919388\n",
      "Epoch: 58, Iteration: 0, Loss: 0.8908646106719971\n",
      "Epoch: 58, Iteration: 100, Loss: 1.5779117345809937\n",
      "Epoch: 58, Iteration: 200, Loss: 1.1223653554916382\n",
      "Epoch: 58, Training loss: 1.1879317928984137\n",
      "Epoch: 59, Iteration: 0, Loss: 0.9059945940971375\n",
      "Epoch: 59, Iteration: 100, Loss: 1.5432775020599365\n",
      "Epoch: 59, Iteration: 200, Loss: 1.0681079626083374\n",
      "Epoch: 59, Training loss: 1.1762927815125752\n",
      "Epoch: 60, Iteration: 0, Loss: 0.8655058145523071\n",
      "Epoch: 60, Iteration: 100, Loss: 1.548988699913025\n",
      "Epoch: 60, Iteration: 200, Loss: 1.060031771659851\n",
      "Epoch: 60, Training loss: 1.1593602140284074\n",
      "Evaluation loss: 4.261290497028986\n",
      "Epoch: 61, Iteration: 0, Loss: 0.8579285740852356\n",
      "Epoch: 61, Iteration: 100, Loss: 1.4868420362472534\n",
      "Epoch: 61, Iteration: 200, Loss: 1.0849952697753906\n",
      "Epoch: 61, Training loss: 1.148523147283655\n",
      "Epoch: 62, Iteration: 0, Loss: 0.8986296653747559\n",
      "Epoch: 62, Iteration: 100, Loss: 1.543876051902771\n",
      "Epoch: 62, Iteration: 200, Loss: 1.015871286392212\n",
      "Epoch: 62, Training loss: 1.1352020048714249\n",
      "Epoch: 63, Iteration: 0, Loss: 0.8540931344032288\n",
      "Epoch: 63, Iteration: 100, Loss: 1.5138585567474365\n",
      "Epoch: 63, Iteration: 200, Loss: 1.0323020219802856\n",
      "Epoch: 63, Training loss: 1.1161928203336604\n",
      "Epoch: 64, Iteration: 0, Loss: 0.7856073379516602\n",
      "Epoch: 64, Iteration: 100, Loss: 1.5297819375991821\n",
      "Epoch: 64, Iteration: 200, Loss: 0.9945833683013916\n",
      "Epoch: 64, Training loss: 1.1015344190217264\n",
      "Epoch: 65, Iteration: 0, Loss: 0.8210152983665466\n",
      "Epoch: 65, Iteration: 100, Loss: 1.4455307722091675\n",
      "Epoch: 65, Iteration: 200, Loss: 0.9922268986701965\n",
      "Epoch: 65, Training loss: 1.08994816997241\n",
      "Epoch: 66, Iteration: 0, Loss: 0.8394436240196228\n",
      "Epoch: 66, Iteration: 100, Loss: 1.4135279655456543\n",
      "Epoch: 66, Iteration: 200, Loss: 0.973987877368927\n",
      "Epoch: 66, Training loss: 1.078635914344083\n",
      "Epoch: 67, Iteration: 0, Loss: 0.7860872149467468\n",
      "Epoch: 67, Iteration: 100, Loss: 1.3927443027496338\n",
      "Epoch: 67, Iteration: 200, Loss: 0.973620593547821\n",
      "Epoch: 67, Training loss: 1.0668248265598022\n",
      "Epoch: 68, Iteration: 0, Loss: 0.758256196975708\n",
      "Epoch: 68, Iteration: 100, Loss: 1.402935266494751\n",
      "Epoch: 68, Iteration: 200, Loss: 0.9447746872901917\n",
      "Epoch: 68, Training loss: 1.0524395954058592\n",
      "Epoch: 69, Iteration: 0, Loss: 0.8154007792472839\n",
      "Epoch: 69, Iteration: 100, Loss: 1.410176157951355\n",
      "Epoch: 69, Iteration: 200, Loss: 0.9208941459655762\n",
      "Epoch: 69, Training loss: 1.0407189987687044\n",
      "Epoch: 70, Iteration: 0, Loss: 0.7902075052261353\n",
      "Epoch: 70, Iteration: 100, Loss: 1.342047095298767\n",
      "Epoch: 70, Iteration: 200, Loss: 0.9648389220237732\n",
      "Epoch: 70, Training loss: 1.0303511602755315\n",
      "Evaluation loss: 4.365248916441934\n",
      "Epoch: 71, Iteration: 0, Loss: 0.7594128251075745\n",
      "Epoch: 71, Iteration: 100, Loss: 1.3440027236938477\n",
      "Epoch: 71, Iteration: 200, Loss: 0.9240212440490723\n",
      "Epoch: 71, Training loss: 1.0152536948457285\n",
      "Epoch: 72, Iteration: 0, Loss: 0.7844399809837341\n",
      "Epoch: 72, Iteration: 100, Loss: 1.4048562049865723\n",
      "Epoch: 72, Iteration: 200, Loss: 0.9256328344345093\n",
      "Epoch: 72, Training loss: 1.0063096200275108\n",
      "Epoch: 73, Iteration: 0, Loss: 0.7772248387336731\n",
      "Epoch: 73, Iteration: 100, Loss: 1.2908519506454468\n",
      "Epoch: 73, Iteration: 200, Loss: 0.9478753805160522\n",
      "Epoch: 73, Training loss: 0.997866292255353\n",
      "Epoch: 74, Iteration: 0, Loss: 0.7432212233543396\n",
      "Epoch: 74, Iteration: 100, Loss: 1.303538203239441\n",
      "Epoch: 74, Iteration: 200, Loss: 0.8966036438941956\n",
      "Epoch: 74, Training loss: 0.9851836678729542\n",
      "Epoch: 75, Iteration: 0, Loss: 0.74432772397995\n",
      "Epoch: 75, Iteration: 100, Loss: 1.3060505390167236\n",
      "Epoch: 75, Iteration: 200, Loss: 0.8664838075637817\n",
      "Epoch: 75, Training loss: 0.9717703266673705\n",
      "Epoch: 76, Iteration: 0, Loss: 0.6995053291320801\n",
      "Epoch: 76, Iteration: 100, Loss: 1.2846399545669556\n",
      "Epoch: 76, Iteration: 200, Loss: 0.8744776248931885\n",
      "Epoch: 76, Training loss: 0.9632087347167516\n",
      "Epoch: 77, Iteration: 0, Loss: 0.667493999004364\n",
      "Epoch: 77, Iteration: 100, Loss: 1.2646358013153076\n",
      "Epoch: 77, Iteration: 200, Loss: 0.8251006007194519\n",
      "Epoch: 77, Training loss: 0.9523239416231525\n",
      "Epoch: 78, Iteration: 0, Loss: 0.6818912029266357\n",
      "Epoch: 78, Iteration: 100, Loss: 1.270257830619812\n",
      "Epoch: 78, Iteration: 200, Loss: 0.9018101692199707\n",
      "Epoch: 78, Training loss: 0.9420311435963555\n",
      "Epoch: 79, Iteration: 0, Loss: 0.7203655242919922\n",
      "Epoch: 79, Iteration: 100, Loss: 1.2506167888641357\n",
      "Epoch: 79, Iteration: 200, Loss: 0.8357523679733276\n",
      "Epoch: 79, Training loss: 0.9341708966166561\n",
      "Epoch: 80, Iteration: 0, Loss: 0.6629704236984253\n",
      "Epoch: 80, Iteration: 100, Loss: 1.3099801540374756\n",
      "Epoch: 80, Iteration: 200, Loss: 0.821475088596344\n",
      "Epoch: 80, Training loss: 0.9261926935039896\n",
      "Evaluation loss: 4.450169146613547\n",
      "Epoch: 81, Iteration: 0, Loss: 0.709253191947937\n",
      "Epoch: 81, Iteration: 100, Loss: 1.204985499382019\n",
      "Epoch: 81, Iteration: 200, Loss: 0.8120812773704529\n",
      "Epoch: 81, Training loss: 0.9154582831933825\n",
      "Epoch: 82, Iteration: 0, Loss: 0.677950918674469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Iteration: 100, Loss: 1.2232677936553955\n",
      "Epoch: 82, Iteration: 200, Loss: 0.7761495113372803\n",
      "Epoch: 82, Training loss: 0.9018916189482964\n",
      "Epoch: 83, Iteration: 0, Loss: 0.6450355052947998\n",
      "Epoch: 83, Iteration: 100, Loss: 1.2015255689620972\n",
      "Epoch: 83, Iteration: 200, Loss: 0.8658823370933533\n",
      "Epoch: 83, Training loss: 0.8966411729968455\n",
      "Epoch: 84, Iteration: 0, Loss: 0.6306508183479309\n",
      "Epoch: 84, Iteration: 100, Loss: 1.1742701530456543\n",
      "Epoch: 84, Iteration: 200, Loss: 0.7904707789421082\n",
      "Epoch: 84, Training loss: 0.8889321643531246\n",
      "Epoch: 85, Iteration: 0, Loss: 0.6847953796386719\n",
      "Epoch: 85, Iteration: 100, Loss: 1.1852107048034668\n",
      "Epoch: 85, Iteration: 200, Loss: 0.7653084993362427\n",
      "Epoch: 85, Training loss: 0.8796683813478496\n",
      "Epoch: 86, Iteration: 0, Loss: 0.6067969799041748\n",
      "Epoch: 86, Iteration: 100, Loss: 1.156286597251892\n",
      "Epoch: 86, Iteration: 200, Loss: 0.7906332612037659\n",
      "Epoch: 86, Training loss: 0.868869749648312\n",
      "Epoch: 87, Iteration: 0, Loss: 0.6766155958175659\n",
      "Epoch: 87, Iteration: 100, Loss: 1.1516309976577759\n",
      "Epoch: 87, Iteration: 200, Loss: 0.7787250876426697\n",
      "Epoch: 87, Training loss: 0.8583756212032266\n",
      "Epoch: 88, Iteration: 0, Loss: 0.6454931497573853\n",
      "Epoch: 88, Iteration: 100, Loss: 1.1184262037277222\n",
      "Epoch: 88, Iteration: 200, Loss: 0.74480140209198\n",
      "Epoch: 88, Training loss: 0.8524382607129518\n",
      "Epoch: 89, Iteration: 0, Loss: 0.6353232264518738\n",
      "Epoch: 89, Iteration: 100, Loss: 1.170937418937683\n",
      "Epoch: 89, Iteration: 200, Loss: 0.7419202327728271\n",
      "Epoch: 89, Training loss: 0.8448156740013606\n",
      "Epoch: 90, Iteration: 0, Loss: 0.5748171210289001\n",
      "Epoch: 90, Iteration: 100, Loss: 1.1319904327392578\n",
      "Epoch: 90, Iteration: 200, Loss: 0.7214683294296265\n",
      "Epoch: 90, Training loss: 0.8332363150419876\n",
      "Evaluation loss: 4.541238628581832\n",
      "Epoch: 91, Iteration: 0, Loss: 0.6134455800056458\n",
      "Epoch: 91, Iteration: 100, Loss: 1.1039574146270752\n",
      "Epoch: 91, Iteration: 200, Loss: 0.7209430932998657\n",
      "Epoch: 91, Training loss: 0.8273545476765918\n",
      "Epoch: 92, Iteration: 0, Loss: 0.5898680090904236\n",
      "Epoch: 92, Iteration: 100, Loss: 1.144370198249817\n",
      "Epoch: 92, Iteration: 200, Loss: 0.7182489037513733\n",
      "Epoch: 92, Training loss: 0.8180800441215774\n",
      "Epoch: 93, Iteration: 0, Loss: 0.6150004863739014\n",
      "Epoch: 93, Iteration: 100, Loss: 1.0352197885513306\n",
      "Epoch: 93, Iteration: 200, Loss: 0.7898722887039185\n",
      "Epoch: 93, Training loss: 0.8077779065729608\n",
      "Epoch: 94, Iteration: 0, Loss: 0.5939223766326904\n",
      "Epoch: 94, Iteration: 100, Loss: 1.0785712003707886\n",
      "Epoch: 94, Iteration: 200, Loss: 0.7208552360534668\n",
      "Epoch: 94, Training loss: 0.8052041914626609\n",
      "Epoch: 95, Iteration: 0, Loss: 0.6471148729324341\n",
      "Epoch: 95, Iteration: 100, Loss: 1.0819321870803833\n",
      "Epoch: 95, Iteration: 200, Loss: 0.7129446268081665\n",
      "Epoch: 95, Training loss: 0.7965422154275612\n",
      "Epoch: 96, Iteration: 0, Loss: 0.5549837350845337\n",
      "Epoch: 96, Iteration: 100, Loss: 1.0725325345993042\n",
      "Epoch: 96, Iteration: 200, Loss: 0.6832402348518372\n",
      "Epoch: 96, Training loss: 0.7877122101746064\n",
      "Epoch: 97, Iteration: 0, Loss: 0.5811020731925964\n",
      "Epoch: 97, Iteration: 100, Loss: 1.0204148292541504\n",
      "Epoch: 97, Iteration: 200, Loss: 0.6717945337295532\n",
      "Epoch: 97, Training loss: 0.7832823858405945\n",
      "Epoch: 98, Iteration: 0, Loss: 0.5489082336425781\n",
      "Epoch: 98, Iteration: 100, Loss: 1.0289477109909058\n",
      "Epoch: 98, Iteration: 200, Loss: 0.6898281574249268\n",
      "Epoch: 98, Training loss: 0.7768379408828912\n",
      "Epoch: 99, Iteration: 0, Loss: 0.5574450492858887\n",
      "Epoch: 99, Iteration: 100, Loss: 1.0500118732452393\n",
      "Epoch: 99, Iteration: 200, Loss: 0.676162838935852\n",
      "Epoch: 99, Training loss: 0.76692129705315\n"
     ]
    }
   ],
   "source": [
    "train(model=model, data=train_data, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = ' '.join([inv_en_dict[w] for w in dev_en[i]])  # inv_cn_dict -> inv_en_dict\n",
    "    cn_sent = ' '.join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(f\"English: {en_sent}\")  # en_dict -> en_sent\n",
    "    print(f\"Chinese: {' '.join(cn_sent)}\")\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict['BOS']]]).long().to(device)\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != 'EOS':\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Translation: {''.join(trans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: BOS you have nice skin . EOS\n",
      "Chinese: B O S   你   的   皮 肤   真   好   。   E O S\n",
      "Translation: 你有一双鞋。\n",
      "==============================\n",
      "English: BOS you 're UNK correct . EOS\n",
      "Chinese: B O S   你   U N K   正 确   。   E O S\n",
      "Translation: 你是不可抗拒的。\n",
      "==============================\n",
      "English: BOS everyone admired his courage . EOS\n",
      "Chinese: B O S   每 个   人   都   佩 服   他   的   勇 气   。   E O S\n",
      "Translation: 每个人都在谈论他的事。\n",
      "==============================\n",
      "English: BOS what time is it ? EOS\n",
      "Chinese: B O S   几 点   了   ？   E O S\n",
      "Translation: 怎么玩呢？\n",
      "==============================\n",
      "English: BOS i 'm free tonight . EOS\n",
      "Chinese: B O S   我   今 晚   有 空   。   E O S\n",
      "Translation: 我今晚有空。\n",
      "==============================\n",
      "English: BOS here is your book . EOS\n",
      "Chinese: B O S   这   是   你   的   书   。   E O S\n",
      "Translation: 你的书在这里。\n",
      "==============================\n",
      "English: BOS they are at lunch . EOS\n",
      "Chinese: B O S   他 们   在   吃   午 饭   。   E O S\n",
      "Translation: 他们在谈得很愉快。\n",
      "==============================\n",
      "English: BOS this chair is UNK . EOS\n",
      "Chinese: B O S   这   把   椅 子   U N K   。   E O S\n",
      "Translation: 这个是很好的。\n",
      "==============================\n",
      "English: BOS it 's pretty heavy . EOS\n",
      "Chinese: B O S   它   U N K   。   E O S\n",
      "Translation: 天快要下雨了。\n",
      "==============================\n",
      "English: BOS many attended his funeral . EOS\n",
      "Chinese: B O S   很 多   人   都   参 加   了   他   的   葬 礼   。   E O S\n",
      "Translation: 许多移民改了来。\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 110):\n",
    "    translate_dev(i)\n",
    "    print('===' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LuongAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Buinding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        packed_embedded = pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "        return out, hid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 根据context vectors和当前的输出hidden states，计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.linear_in = nn.Linear(enc_hidden_size * 2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size * 2 + dec_hidden_size, dec_hidden_size)\n",
    "    \n",
    "    def forward(self, output, context, mask):\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        context_in = self.linear_in(context.view(batch_size * input_len, -1)).view(batch_size, input_len, -1)\n",
    "        attn = torch.bmm(output, context_in.transpose(1, 2))\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        context = torch.bmm(attn, context)\n",
    "        output = torch.cat((context, output), dim=2)\n",
    "        output = output.view(batch_size * output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- decoder会根据已经翻译好的句子内容，和context vectors来决定下一个输出的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_mask(self, x_len, y_len):\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))\n",
    "        packed_seq = pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)  # 在类中，可以在别的函数里调用其他函数\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 最后我们构建Seq2Seq模型把encoder，attention和decoder串到一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, ctx_lengths=x_lengths, y=y, y_lengths=y_lengths, hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, ctx_lengths=x_lengths, y=y, y_lengths=torch.ones(batch_size).long().to(y.device), hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words, embed_size=embed_size, enc_hidden_size=hidden_size, dec_hidden_size=hidden_size, dropout=dropout)\n",
    "model_attn = Seq2Seq(encoder=encoder, decoder=decoder)\n",
    "model_attn = model_attn.to(device)  # RuntimeError: Expected object of backend CPU but got backend CUDA for argument #3 'index'\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = optim.Adam(model_attn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 9.083841323852539\n",
      "Epoch: 0, Iteration: 100, Loss: 5.233992099761963\n",
      "Epoch: 0, Iteration: 200, Loss: 5.036600112915039\n",
      "Epoch: 0, Training loss: 5.619496361293119\n",
      "Evaluation loss: 5.0174953266245\n",
      "Epoch: 1, Iteration: 0, Loss: 5.418123245239258\n",
      "Epoch: 1, Iteration: 100, Loss: 4.573816299438477\n",
      "Epoch: 1, Iteration: 200, Loss: 4.568813323974609\n",
      "Epoch: 1, Training loss: 4.793363921525538\n",
      "Epoch: 2, Iteration: 0, Loss: 4.965517044067383\n",
      "Epoch: 2, Iteration: 100, Loss: 4.154123783111572\n",
      "Epoch: 2, Iteration: 200, Loss: 4.20722770690918\n",
      "Epoch: 2, Training loss: 4.404310847049115\n",
      "Epoch: 3, Iteration: 0, Loss: 4.626755714416504\n",
      "Epoch: 3, Iteration: 100, Loss: 3.8436853885650635\n",
      "Epoch: 3, Iteration: 200, Loss: 3.9486076831817627\n",
      "Epoch: 3, Training loss: 4.12138103870595\n",
      "Epoch: 4, Iteration: 0, Loss: 4.387110233306885\n",
      "Epoch: 4, Iteration: 100, Loss: 3.6054558753967285\n",
      "Epoch: 4, Iteration: 200, Loss: 3.7147574424743652\n",
      "Epoch: 4, Training loss: 3.8913391629439125\n",
      "Epoch: 5, Iteration: 0, Loss: 4.195049285888672\n",
      "Epoch: 5, Iteration: 100, Loss: 3.384795904159546\n",
      "Epoch: 5, Iteration: 200, Loss: 3.5285146236419678\n",
      "Epoch: 5, Training loss: 3.6906737866672406\n",
      "Epoch: 6, Iteration: 0, Loss: 3.980085849761963\n",
      "Epoch: 6, Iteration: 100, Loss: 3.2172350883483887\n",
      "Epoch: 6, Iteration: 200, Loss: 3.3217544555664062\n",
      "Epoch: 6, Training loss: 3.5112832086911836\n",
      "Epoch: 7, Iteration: 0, Loss: 3.8533198833465576\n",
      "Epoch: 7, Iteration: 100, Loss: 3.0448358058929443\n",
      "Epoch: 7, Iteration: 200, Loss: 3.1596083641052246\n",
      "Epoch: 7, Training loss: 3.354068187117805\n",
      "Epoch: 8, Iteration: 0, Loss: 3.7126710414886475\n",
      "Epoch: 8, Iteration: 100, Loss: 2.9141132831573486\n",
      "Epoch: 8, Iteration: 200, Loss: 3.0239150524139404\n",
      "Epoch: 8, Training loss: 3.2062821723724206\n",
      "Epoch: 9, Iteration: 0, Loss: 3.5822670459747314\n",
      "Epoch: 9, Iteration: 100, Loss: 2.7992255687713623\n",
      "Epoch: 9, Iteration: 200, Loss: 2.886324405670166\n",
      "Epoch: 9, Training loss: 3.0698049618955774\n",
      "Epoch: 10, Iteration: 0, Loss: 3.436544179916382\n",
      "Epoch: 10, Iteration: 100, Loss: 2.642416000366211\n",
      "Epoch: 10, Iteration: 200, Loss: 2.74810528755188\n",
      "Epoch: 10, Training loss: 2.940304689510417\n",
      "Evaluation loss: 3.746080411992082\n",
      "Epoch: 11, Iteration: 0, Loss: 3.3486709594726562\n",
      "Epoch: 11, Iteration: 100, Loss: 2.5358102321624756\n",
      "Epoch: 11, Iteration: 200, Loss: 2.630697011947632\n",
      "Epoch: 11, Training loss: 2.822379838115899\n",
      "Epoch: 12, Iteration: 0, Loss: 3.233764410018921\n",
      "Epoch: 12, Iteration: 100, Loss: 2.411466121673584\n",
      "Epoch: 12, Iteration: 200, Loss: 2.5493602752685547\n",
      "Epoch: 12, Training loss: 2.7084842183193136\n",
      "Epoch: 13, Iteration: 0, Loss: 3.134899139404297\n",
      "Epoch: 13, Iteration: 100, Loss: 2.3073816299438477\n",
      "Epoch: 13, Iteration: 200, Loss: 2.4187545776367188\n",
      "Epoch: 13, Training loss: 2.6057151532118623\n",
      "Epoch: 14, Iteration: 0, Loss: 3.014936923980713\n",
      "Epoch: 14, Iteration: 100, Loss: 2.2157249450683594\n",
      "Epoch: 14, Iteration: 200, Loss: 2.2990338802337646\n",
      "Epoch: 14, Training loss: 2.50195698233452\n",
      "Epoch: 15, Iteration: 0, Loss: 2.9122111797332764\n",
      "Epoch: 15, Iteration: 100, Loss: 2.136530876159668\n",
      "Epoch: 15, Iteration: 200, Loss: 2.1980702877044678\n",
      "Epoch: 15, Training loss: 2.408010511507091\n",
      "Epoch: 16, Iteration: 0, Loss: 2.801004648208618\n",
      "Epoch: 16, Iteration: 100, Loss: 2.0405077934265137\n",
      "Epoch: 16, Iteration: 200, Loss: 2.1152145862579346\n",
      "Epoch: 16, Training loss: 2.317344457795708\n",
      "Epoch: 17, Iteration: 0, Loss: 2.7349331378936768\n",
      "Epoch: 17, Iteration: 100, Loss: 1.997209072113037\n",
      "Epoch: 17, Iteration: 200, Loss: 2.0213840007781982\n",
      "Epoch: 17, Training loss: 2.228259505160405\n",
      "Epoch: 18, Iteration: 0, Loss: 2.651390552520752\n",
      "Epoch: 18, Iteration: 100, Loss: 1.8822331428527832\n",
      "Epoch: 18, Iteration: 200, Loss: 1.9164146184921265\n",
      "Epoch: 18, Training loss: 2.1500961443747397\n",
      "Epoch: 19, Iteration: 0, Loss: 2.552535057067871\n",
      "Epoch: 19, Iteration: 100, Loss: 1.751360297203064\n",
      "Epoch: 19, Iteration: 200, Loss: 1.886957049369812\n",
      "Epoch: 19, Training loss: 2.071542782997931\n",
      "Epoch: 20, Iteration: 0, Loss: 2.4799084663391113\n",
      "Epoch: 20, Iteration: 100, Loss: 1.73977792263031\n",
      "Epoch: 20, Iteration: 200, Loss: 1.750693917274475\n",
      "Epoch: 20, Training loss: 1.9958523403465034\n",
      "Evaluation loss: 3.6340241236993442\n",
      "Epoch: 21, Iteration: 0, Loss: 2.3768882751464844\n",
      "Epoch: 21, Iteration: 100, Loss: 1.6437159776687622\n",
      "Epoch: 21, Iteration: 200, Loss: 1.7097140550613403\n",
      "Epoch: 21, Training loss: 1.9281404273404417\n",
      "Epoch: 22, Iteration: 0, Loss: 2.3031857013702393\n",
      "Epoch: 22, Iteration: 100, Loss: 1.5913233757019043\n",
      "Epoch: 22, Iteration: 200, Loss: 1.6459965705871582\n",
      "Epoch: 22, Training loss: 1.8629719338868813\n",
      "Epoch: 23, Iteration: 0, Loss: 2.228309154510498\n",
      "Epoch: 23, Iteration: 100, Loss: 1.5704317092895508\n",
      "Epoch: 23, Iteration: 200, Loss: 1.5901371240615845\n",
      "Epoch: 23, Training loss: 1.8009971842281793\n",
      "Epoch: 24, Iteration: 0, Loss: 2.1646955013275146\n",
      "Epoch: 24, Iteration: 100, Loss: 1.4692825078964233\n",
      "Epoch: 24, Iteration: 200, Loss: 1.5820640325546265\n",
      "Epoch: 24, Training loss: 1.7405708577177128\n",
      "Epoch: 25, Iteration: 0, Loss: 2.1067442893981934\n",
      "Epoch: 25, Iteration: 100, Loss: 1.4651588201522827\n",
      "Epoch: 25, Iteration: 200, Loss: 1.5375034809112549\n",
      "Epoch: 25, Training loss: 1.684355031246327\n",
      "Epoch: 26, Iteration: 0, Loss: 2.0596137046813965\n",
      "Epoch: 26, Iteration: 100, Loss: 1.3938665390014648\n",
      "Epoch: 26, Iteration: 200, Loss: 1.4380730390548706\n",
      "Epoch: 26, Training loss: 1.633931203868632\n",
      "Epoch: 27, Iteration: 0, Loss: 1.9766771793365479\n",
      "Epoch: 27, Iteration: 100, Loss: 1.3085343837738037\n",
      "Epoch: 27, Iteration: 200, Loss: 1.456681489944458\n",
      "Epoch: 27, Training loss: 1.5828722907294068\n",
      "Epoch: 28, Iteration: 0, Loss: 1.9648767709732056\n",
      "Epoch: 28, Iteration: 100, Loss: 1.2605602741241455\n",
      "Epoch: 28, Iteration: 200, Loss: 1.3724894523620605\n",
      "Epoch: 28, Training loss: 1.5359518244181916\n",
      "Epoch: 29, Iteration: 0, Loss: 1.9001164436340332\n",
      "Epoch: 29, Iteration: 100, Loss: 1.2670332193374634\n",
      "Epoch: 29, Iteration: 200, Loss: 1.3215441703796387\n",
      "Epoch: 29, Training loss: 1.4916017056288293\n",
      "Epoch: 30, Iteration: 0, Loss: 1.8127397298812866\n",
      "Epoch: 30, Iteration: 100, Loss: 1.2179685831069946\n",
      "Epoch: 30, Iteration: 200, Loss: 1.325368046760559\n",
      "Epoch: 30, Training loss: 1.4451574814513868\n",
      "Evaluation loss: 3.6960601666364394\n",
      "Epoch: 31, Iteration: 0, Loss: 1.8035305738449097\n",
      "Epoch: 31, Iteration: 100, Loss: 1.1600440740585327\n",
      "Epoch: 31, Iteration: 200, Loss: 1.224074125289917\n",
      "Epoch: 31, Training loss: 1.4079667464510601\n",
      "Epoch: 32, Iteration: 0, Loss: 1.7044801712036133\n",
      "Epoch: 32, Iteration: 100, Loss: 1.134428858757019\n",
      "Epoch: 32, Iteration: 200, Loss: 1.1632335186004639\n",
      "Epoch: 32, Training loss: 1.3667174499397101\n",
      "Epoch: 33, Iteration: 0, Loss: 1.7669687271118164\n",
      "Epoch: 33, Iteration: 100, Loss: 1.0515542030334473\n",
      "Epoch: 33, Iteration: 200, Loss: 1.1117907762527466\n",
      "Epoch: 33, Training loss: 1.3311819686742699\n",
      "Epoch: 34, Iteration: 0, Loss: 1.6479727029800415\n",
      "Epoch: 34, Iteration: 100, Loss: 1.0253709554672241\n",
      "Epoch: 34, Iteration: 200, Loss: 1.1200344562530518\n",
      "Epoch: 34, Training loss: 1.2956013767617722\n",
      "Epoch: 35, Iteration: 0, Loss: 1.5747456550598145\n",
      "Epoch: 35, Iteration: 100, Loss: 1.0128569602966309\n",
      "Epoch: 35, Iteration: 200, Loss: 1.1173114776611328\n",
      "Epoch: 35, Training loss: 1.264937369882029\n",
      "Epoch: 36, Iteration: 0, Loss: 1.6231684684753418\n",
      "Epoch: 36, Iteration: 100, Loss: 1.025309681892395\n",
      "Epoch: 36, Iteration: 200, Loss: 1.0632332563400269\n",
      "Epoch: 36, Training loss: 1.2378911032178328\n",
      "Epoch: 37, Iteration: 0, Loss: 1.5594568252563477\n",
      "Epoch: 37, Iteration: 100, Loss: 0.9659059643745422\n",
      "Epoch: 37, Iteration: 200, Loss: 1.0103731155395508\n",
      "Epoch: 37, Training loss: 1.2023466712159674\n",
      "Epoch: 38, Iteration: 0, Loss: 1.4960215091705322\n",
      "Epoch: 38, Iteration: 100, Loss: 0.9177150726318359\n",
      "Epoch: 38, Iteration: 200, Loss: 0.9912070035934448\n",
      "Epoch: 38, Training loss: 1.1710632969994217\n",
      "Epoch: 39, Iteration: 0, Loss: 1.5143221616744995\n",
      "Epoch: 39, Iteration: 100, Loss: 0.8921533226966858\n",
      "Epoch: 39, Iteration: 200, Loss: 0.9713610410690308\n",
      "Epoch: 39, Training loss: 1.147166995966971\n",
      "Epoch: 40, Iteration: 0, Loss: 1.439597249031067\n",
      "Epoch: 40, Iteration: 100, Loss: 0.8617582321166992\n",
      "Epoch: 40, Iteration: 200, Loss: 0.9809545278549194\n",
      "Epoch: 40, Training loss: 1.120209349389226\n",
      "Evaluation loss: 3.810876260723984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Iteration: 0, Loss: 1.3922542333602905\n",
      "Epoch: 41, Iteration: 100, Loss: 0.865725576877594\n",
      "Epoch: 41, Iteration: 200, Loss: 0.9584619402885437\n",
      "Epoch: 41, Training loss: 1.0918338428544667\n",
      "Epoch: 42, Iteration: 0, Loss: 1.404598355293274\n",
      "Epoch: 42, Iteration: 100, Loss: 0.8189520239830017\n",
      "Epoch: 42, Iteration: 200, Loss: 0.9348135590553284\n",
      "Epoch: 42, Training loss: 1.0675075024824014\n",
      "Epoch: 43, Iteration: 0, Loss: 1.3821415901184082\n",
      "Epoch: 43, Iteration: 100, Loss: 0.8094674348831177\n",
      "Epoch: 43, Iteration: 200, Loss: 0.867322564125061\n",
      "Epoch: 43, Training loss: 1.048121057894403\n",
      "Epoch: 44, Iteration: 0, Loss: 1.3603922128677368\n",
      "Epoch: 44, Iteration: 100, Loss: 0.746292769908905\n",
      "Epoch: 44, Iteration: 200, Loss: 0.8881151080131531\n",
      "Epoch: 44, Training loss: 1.0183486800628403\n",
      "Epoch: 45, Iteration: 0, Loss: 1.3183605670928955\n",
      "Epoch: 45, Iteration: 100, Loss: 0.7634983062744141\n",
      "Epoch: 45, Iteration: 200, Loss: 0.9116721749305725\n",
      "Epoch: 45, Training loss: 0.9990173028967627\n",
      "Epoch: 46, Iteration: 0, Loss: 1.2513562440872192\n",
      "Epoch: 46, Iteration: 100, Loss: 0.7348352074623108\n",
      "Epoch: 46, Iteration: 200, Loss: 0.8082109689712524\n",
      "Epoch: 46, Training loss: 0.9738204321494461\n",
      "Epoch: 47, Iteration: 0, Loss: 1.2388718128204346\n",
      "Epoch: 47, Iteration: 100, Loss: 0.7050294280052185\n",
      "Epoch: 47, Iteration: 200, Loss: 0.8622862100601196\n",
      "Epoch: 47, Training loss: 0.9541331402060664\n",
      "Epoch: 48, Iteration: 0, Loss: 1.239710807800293\n",
      "Epoch: 48, Iteration: 100, Loss: 0.6876392960548401\n",
      "Epoch: 48, Iteration: 200, Loss: 0.8182026743888855\n",
      "Epoch: 48, Training loss: 0.9338326871118204\n",
      "Epoch: 49, Iteration: 0, Loss: 1.1903736591339111\n",
      "Epoch: 49, Iteration: 100, Loss: 0.7220053672790527\n",
      "Epoch: 49, Iteration: 200, Loss: 0.7530402541160583\n",
      "Epoch: 49, Training loss: 0.9167519778732446\n",
      "Epoch: 50, Iteration: 0, Loss: 1.1997730731964111\n",
      "Epoch: 50, Iteration: 100, Loss: 0.7019183039665222\n",
      "Epoch: 50, Iteration: 200, Loss: 0.7782084345817566\n",
      "Epoch: 50, Training loss: 0.8990630325968616\n",
      "Evaluation loss: 3.943111333590638\n",
      "Epoch: 51, Iteration: 0, Loss: 1.1336877346038818\n",
      "Epoch: 51, Iteration: 100, Loss: 0.6415071487426758\n",
      "Epoch: 51, Iteration: 200, Loss: 0.7456146478652954\n",
      "Epoch: 51, Training loss: 0.8821759167407323\n",
      "Epoch: 52, Iteration: 0, Loss: 1.1500228643417358\n",
      "Epoch: 52, Iteration: 100, Loss: 0.5985183119773865\n",
      "Epoch: 52, Iteration: 200, Loss: 0.738591730594635\n",
      "Epoch: 52, Training loss: 0.8634121325005728\n",
      "Epoch: 53, Iteration: 0, Loss: 1.1038330793380737\n",
      "Epoch: 53, Iteration: 100, Loss: 0.6447497606277466\n",
      "Epoch: 53, Iteration: 200, Loss: 0.7732803821563721\n",
      "Epoch: 53, Training loss: 0.8469982890935243\n",
      "Epoch: 54, Iteration: 0, Loss: 1.107261061668396\n",
      "Epoch: 54, Iteration: 100, Loss: 0.633383572101593\n",
      "Epoch: 54, Iteration: 200, Loss: 0.7257184982299805\n",
      "Epoch: 54, Training loss: 0.8310646047364254\n",
      "Epoch: 55, Iteration: 0, Loss: 1.0503730773925781\n",
      "Epoch: 55, Iteration: 100, Loss: 0.5792661309242249\n",
      "Epoch: 55, Iteration: 200, Loss: 0.6989463567733765\n",
      "Epoch: 55, Training loss: 0.8128079397046225\n",
      "Epoch: 56, Iteration: 0, Loss: 1.0999432802200317\n",
      "Epoch: 56, Iteration: 100, Loss: 0.5811485648155212\n",
      "Epoch: 56, Iteration: 200, Loss: 0.6787273287773132\n",
      "Epoch: 56, Training loss: 0.7987245673643885\n",
      "Epoch: 57, Iteration: 0, Loss: 1.0811654329299927\n",
      "Epoch: 57, Iteration: 100, Loss: 0.5962480306625366\n",
      "Epoch: 57, Iteration: 200, Loss: 0.6591185331344604\n",
      "Epoch: 57, Training loss: 0.7856219603952687\n",
      "Epoch: 58, Iteration: 0, Loss: 1.059958577156067\n",
      "Epoch: 58, Iteration: 100, Loss: 0.5988269448280334\n",
      "Epoch: 58, Iteration: 200, Loss: 0.7027915716171265\n",
      "Epoch: 58, Training loss: 0.7703351832933407\n",
      "Epoch: 59, Iteration: 0, Loss: 0.9706425666809082\n",
      "Epoch: 59, Iteration: 100, Loss: 0.4928438663482666\n",
      "Epoch: 59, Iteration: 200, Loss: 0.6682353019714355\n",
      "Epoch: 59, Training loss: 0.7524689579020156\n",
      "Epoch: 60, Iteration: 0, Loss: 1.0266422033309937\n",
      "Epoch: 60, Iteration: 100, Loss: 0.5224523544311523\n",
      "Epoch: 60, Iteration: 200, Loss: 0.6162432432174683\n",
      "Epoch: 60, Training loss: 0.7373965700833466\n",
      "Evaluation loss: 4.07254528958521\n",
      "Epoch: 61, Iteration: 0, Loss: 0.949793815612793\n",
      "Epoch: 61, Iteration: 100, Loss: 0.5376725792884827\n",
      "Epoch: 61, Iteration: 200, Loss: 0.6129891276359558\n",
      "Epoch: 61, Training loss: 0.7277168328956714\n",
      "Epoch: 62, Iteration: 0, Loss: 0.9606252312660217\n",
      "Epoch: 62, Iteration: 100, Loss: 0.5161188244819641\n",
      "Epoch: 62, Iteration: 200, Loss: 0.6042168140411377\n",
      "Epoch: 62, Training loss: 0.7168342024971734\n",
      "Epoch: 63, Iteration: 0, Loss: 0.9213009476661682\n",
      "Epoch: 63, Iteration: 100, Loss: 0.4790455996990204\n",
      "Epoch: 63, Iteration: 200, Loss: 0.6153244972229004\n",
      "Epoch: 63, Training loss: 0.7017270240054687\n",
      "Epoch: 64, Iteration: 0, Loss: 0.9474026560783386\n",
      "Epoch: 64, Iteration: 100, Loss: 0.5147363543510437\n",
      "Epoch: 64, Iteration: 200, Loss: 0.5339547395706177\n",
      "Epoch: 64, Training loss: 0.6922430606308303\n",
      "Epoch: 65, Iteration: 0, Loss: 0.9307299852371216\n",
      "Epoch: 65, Iteration: 100, Loss: 0.4991770386695862\n",
      "Epoch: 65, Iteration: 200, Loss: 0.5812292695045471\n",
      "Epoch: 65, Training loss: 0.6814641803393383\n",
      "Epoch: 66, Iteration: 0, Loss: 0.8644614815711975\n",
      "Epoch: 66, Iteration: 100, Loss: 0.5205196738243103\n",
      "Epoch: 66, Iteration: 200, Loss: 0.5908805727958679\n",
      "Epoch: 66, Training loss: 0.6694744483527678\n",
      "Epoch: 67, Iteration: 0, Loss: 0.8749098181724548\n",
      "Epoch: 67, Iteration: 100, Loss: 0.4927762746810913\n",
      "Epoch: 67, Iteration: 200, Loss: 0.5504603385925293\n",
      "Epoch: 67, Training loss: 0.6566080535465487\n",
      "Epoch: 68, Iteration: 0, Loss: 0.8738515377044678\n",
      "Epoch: 68, Iteration: 100, Loss: 0.4482423663139343\n",
      "Epoch: 68, Iteration: 200, Loss: 0.5539780855178833\n",
      "Epoch: 68, Training loss: 0.647099364771398\n",
      "Epoch: 69, Iteration: 0, Loss: 0.8403672575950623\n",
      "Epoch: 69, Iteration: 100, Loss: 0.4311690330505371\n",
      "Epoch: 69, Iteration: 200, Loss: 0.5712444186210632\n",
      "Epoch: 69, Training loss: 0.6361589538888242\n",
      "Epoch: 70, Iteration: 0, Loss: 0.8406887054443359\n",
      "Epoch: 70, Iteration: 100, Loss: 0.4421292841434479\n",
      "Epoch: 70, Iteration: 200, Loss: 0.5027422308921814\n",
      "Epoch: 70, Training loss: 0.6259710021230869\n",
      "Evaluation loss: 4.200216917266623\n",
      "Epoch: 71, Iteration: 0, Loss: 0.8030722141265869\n",
      "Epoch: 71, Iteration: 100, Loss: 0.4056844115257263\n",
      "Epoch: 71, Iteration: 200, Loss: 0.5177074670791626\n",
      "Epoch: 71, Training loss: 0.6159786562079882\n",
      "Epoch: 72, Iteration: 0, Loss: 0.7942429780960083\n",
      "Epoch: 72, Iteration: 100, Loss: 0.40739816427230835\n",
      "Epoch: 72, Iteration: 200, Loss: 0.5269680023193359\n",
      "Epoch: 72, Training loss: 0.6089102621456786\n",
      "Epoch: 73, Iteration: 0, Loss: 0.7787154912948608\n",
      "Epoch: 73, Iteration: 100, Loss: 0.41910532116889954\n",
      "Epoch: 73, Iteration: 200, Loss: 0.5247822403907776\n",
      "Epoch: 73, Training loss: 0.599291236481277\n",
      "Epoch: 74, Iteration: 0, Loss: 0.8243453502655029\n",
      "Epoch: 74, Iteration: 100, Loss: 0.40945950150489807\n",
      "Epoch: 74, Iteration: 200, Loss: 0.5284473299980164\n",
      "Epoch: 74, Training loss: 0.5888117000372638\n",
      "Epoch: 75, Iteration: 0, Loss: 0.7855193614959717\n",
      "Epoch: 75, Iteration: 100, Loss: 0.3950236439704895\n",
      "Epoch: 75, Iteration: 200, Loss: 0.46235987544059753\n",
      "Epoch: 75, Training loss: 0.5788896850508632\n",
      "Epoch: 76, Iteration: 0, Loss: 0.7929635643959045\n",
      "Epoch: 76, Iteration: 100, Loss: 0.43680432438850403\n",
      "Epoch: 76, Iteration: 200, Loss: 0.4659854769706726\n",
      "Epoch: 76, Training loss: 0.5685684554127186\n",
      "Epoch: 77, Iteration: 0, Loss: 0.8235978484153748\n",
      "Epoch: 77, Iteration: 100, Loss: 0.34883126616477966\n",
      "Epoch: 77, Iteration: 200, Loss: 0.4796292781829834\n",
      "Epoch: 77, Training loss: 0.5611186753007258\n",
      "Epoch: 78, Iteration: 0, Loss: 0.7660672664642334\n",
      "Epoch: 78, Iteration: 100, Loss: 0.3870198130607605\n",
      "Epoch: 78, Iteration: 200, Loss: 0.4521940350532532\n",
      "Epoch: 78, Training loss: 0.556282561043565\n",
      "Epoch: 79, Iteration: 0, Loss: 0.7514647245407104\n",
      "Epoch: 79, Iteration: 100, Loss: 0.4154566526412964\n",
      "Epoch: 79, Iteration: 200, Loss: 0.4718354046344757\n",
      "Epoch: 79, Training loss: 0.5489233918728469\n",
      "Epoch: 80, Iteration: 0, Loss: 0.7166420221328735\n",
      "Epoch: 80, Iteration: 100, Loss: 0.3870331346988678\n",
      "Epoch: 80, Iteration: 200, Loss: 0.4669409394264221\n",
      "Epoch: 80, Training loss: 0.5346482700840484\n",
      "Evaluation loss: 4.316941799238088\n",
      "Epoch: 81, Iteration: 0, Loss: 0.7021794319152832\n",
      "Epoch: 81, Iteration: 100, Loss: 0.332734078168869\n",
      "Epoch: 81, Iteration: 200, Loss: 0.4993208646774292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Training loss: 0.5283196900854777\n",
      "Epoch: 82, Iteration: 0, Loss: 0.6960014700889587\n",
      "Epoch: 82, Iteration: 100, Loss: 0.3309747576713562\n",
      "Epoch: 82, Iteration: 200, Loss: 0.4342208206653595\n",
      "Epoch: 82, Training loss: 0.5223976796397417\n",
      "Epoch: 83, Iteration: 0, Loss: 0.6769198775291443\n",
      "Epoch: 83, Iteration: 100, Loss: 0.34168413281440735\n",
      "Epoch: 83, Iteration: 200, Loss: 0.42325451970100403\n",
      "Epoch: 83, Training loss: 0.5158083271339596\n",
      "Epoch: 84, Iteration: 0, Loss: 0.7354187369346619\n",
      "Epoch: 84, Iteration: 100, Loss: 0.3864680528640747\n",
      "Epoch: 84, Iteration: 200, Loss: 0.497295081615448\n",
      "Epoch: 84, Training loss: 0.5058959165947498\n",
      "Epoch: 85, Iteration: 0, Loss: 0.6709532141685486\n",
      "Epoch: 85, Iteration: 100, Loss: 0.34118950366973877\n",
      "Epoch: 85, Iteration: 200, Loss: 0.4417526125907898\n",
      "Epoch: 85, Training loss: 0.5019440968690109\n",
      "Epoch: 86, Iteration: 0, Loss: 0.676158607006073\n",
      "Epoch: 86, Iteration: 100, Loss: 0.38896089792251587\n",
      "Epoch: 86, Iteration: 200, Loss: 0.4313361942768097\n",
      "Epoch: 86, Training loss: 0.4989966824341623\n",
      "Epoch: 87, Iteration: 0, Loss: 0.6276105046272278\n",
      "Epoch: 87, Iteration: 100, Loss: 0.37612706422805786\n",
      "Epoch: 87, Iteration: 200, Loss: 0.3903287649154663\n",
      "Epoch: 87, Training loss: 0.49142823688206594\n",
      "Epoch: 88, Iteration: 0, Loss: 0.6584839820861816\n",
      "Epoch: 88, Iteration: 100, Loss: 0.30227038264274597\n",
      "Epoch: 88, Iteration: 200, Loss: 0.3769303858280182\n",
      "Epoch: 88, Training loss: 0.48357972636696284\n",
      "Epoch: 89, Iteration: 0, Loss: 0.6425327062606812\n",
      "Epoch: 89, Iteration: 100, Loss: 0.28822463750839233\n",
      "Epoch: 89, Iteration: 200, Loss: 0.4260712265968323\n",
      "Epoch: 89, Training loss: 0.475026856233033\n",
      "Epoch: 90, Iteration: 0, Loss: 0.6310520768165588\n",
      "Epoch: 90, Iteration: 100, Loss: 0.3043774664402008\n",
      "Epoch: 90, Iteration: 200, Loss: 0.4049931764602661\n",
      "Epoch: 90, Training loss: 0.46999909566433035\n",
      "Evaluation loss: 4.439179654502951\n",
      "Epoch: 91, Iteration: 0, Loss: 0.5789103507995605\n",
      "Epoch: 91, Iteration: 100, Loss: 0.3446853458881378\n",
      "Epoch: 91, Iteration: 200, Loss: 0.3986286222934723\n",
      "Epoch: 91, Training loss: 0.4636394341616102\n",
      "Epoch: 92, Iteration: 0, Loss: 0.5826324820518494\n",
      "Epoch: 92, Iteration: 100, Loss: 0.2916829288005829\n",
      "Epoch: 92, Iteration: 200, Loss: 0.41403746604919434\n",
      "Epoch: 92, Training loss: 0.45538022165125996\n",
      "Epoch: 93, Iteration: 0, Loss: 0.6013296842575073\n",
      "Epoch: 93, Iteration: 100, Loss: 0.2939886152744293\n",
      "Epoch: 93, Iteration: 200, Loss: 0.36318913102149963\n",
      "Epoch: 93, Training loss: 0.4473890264700411\n",
      "Epoch: 94, Iteration: 0, Loss: 0.5787369012832642\n",
      "Epoch: 94, Iteration: 100, Loss: 0.3263052701950073\n",
      "Epoch: 94, Iteration: 200, Loss: 0.4058447480201721\n",
      "Epoch: 94, Training loss: 0.4437383944557756\n",
      "Epoch: 95, Iteration: 0, Loss: 0.5884750485420227\n",
      "Epoch: 95, Iteration: 100, Loss: 0.2743980884552002\n",
      "Epoch: 95, Iteration: 200, Loss: 0.3878425359725952\n",
      "Epoch: 95, Training loss: 0.43741873576082474\n",
      "Epoch: 96, Iteration: 0, Loss: 0.5907071232795715\n",
      "Epoch: 96, Iteration: 100, Loss: 0.30668359994888306\n",
      "Epoch: 96, Iteration: 200, Loss: 0.370831161737442\n",
      "Epoch: 96, Training loss: 0.43035467134879657\n",
      "Epoch: 97, Iteration: 0, Loss: 0.5494216680526733\n",
      "Epoch: 97, Iteration: 100, Loss: 0.2918667495250702\n",
      "Epoch: 97, Iteration: 200, Loss: 0.3568158447742462\n",
      "Epoch: 97, Training loss: 0.4245804122805884\n",
      "Epoch: 98, Iteration: 0, Loss: 0.5548830628395081\n",
      "Epoch: 98, Iteration: 100, Loss: 0.2855358123779297\n",
      "Epoch: 98, Iteration: 200, Loss: 0.3406931757926941\n",
      "Epoch: 98, Training loss: 0.42455367716952086\n",
      "Epoch: 99, Iteration: 0, Loss: 0.5785596370697021\n",
      "Epoch: 99, Iteration: 100, Loss: 0.26454004645347595\n",
      "Epoch: 99, Iteration: 200, Loss: 0.306205689907074\n",
      "Epoch: 99, Training loss: 0.4162861565754179\n"
     ]
    }
   ],
   "source": [
    "train(model=model_attn, data=train_data, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev_attn(i):\n",
    "    en_sent = ' '.join([inv_en_dict[w] for w in dev_en[i]])  # inv_cn_dict -> inv_en_dict\n",
    "    cn_sent = ' '.join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(f\"English: {en_sent}\")  # en_dict -> en_sent\n",
    "    print(f\"Chinese: {' '.join(cn_sent)}\")\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict['BOS']]]).long().to(device)\n",
    "    translation, attn = model_attn.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != 'EOS':\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(f\"Translation: {''.join(trans)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: BOS you have nice skin . EOS\n",
      "Chinese: B O S   你   的   皮 肤   真   好   。   E O S\n",
      "Translation: 你有一双迷人的眼睛。\n",
      "==============================\n",
      "English: BOS you 're UNK correct . EOS\n",
      "Chinese: B O S   你   U N K   正 确   。   E O S\n",
      "Translation: 你是要到好。\n",
      "==============================\n",
      "English: BOS everyone admired his courage . EOS\n",
      "Chinese: B O S   每 个   人   都   佩 服   他   的   勇 气   。   E O S\n",
      "Translation: 每个人都能听他的情绪。\n",
      "==============================\n",
      "English: BOS what time is it ? EOS\n",
      "Chinese: B O S   几 点   了   ？   E O S\n",
      "Translation: 怎么看？\n",
      "==============================\n",
      "English: BOS i 'm free tonight . EOS\n",
      "Chinese: B O S   我   今 晚   有 空   。   E O S\n",
      "Translation: 我今晚有空。\n",
      "==============================\n",
      "English: BOS here is your book . EOS\n",
      "Chinese: B O S   这   是   你   的   书   。   E O S\n",
      "Translation: 你的书在这儿。\n",
      "==============================\n",
      "English: BOS they are at lunch . EOS\n",
      "Chinese: B O S   他 们   在   吃   午 饭   。   E O S\n",
      "Translation: 他们正在午餐。\n",
      "==============================\n",
      "English: BOS this chair is UNK . EOS\n",
      "Chinese: B O S   这   把   椅 子   U N K   。   E O S\n",
      "Translation: 这本书是假。\n",
      "==============================\n",
      "English: BOS it 's pretty heavy . EOS\n",
      "Chinese: B O S   它   U N K   。   E O S\n",
      "Translation: 它害怕了。\n",
      "==============================\n",
      "English: BOS many attended his funeral . EOS\n",
      "Chinese: B O S   很 多   人   都   参 加   了   他   的   葬 礼   。   E O S\n",
      "Translation: 这个男孩会开车。\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for i in range(100, 110):\n",
    "    translate_dev_attn(i)\n",
    "    print('===' * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Torch10] *",
   "language": "python",
   "name": "conda-env-Torch10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
