{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "# 设定hyper parameters\n",
    "C = 3  # context window\n",
    "K = 100  # number of negative samples, 每出现一个正确的词就要出现100个错误的词\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.2\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本中读取所有的文字，通过这些文本创建一个vocabulary；\n",
    "- 由于单词数量可能很大，我们选取最常见的MAX_VOCAB_SIZE个单词；\n",
    "- 我们添加一个UNK单词表示所有不常见的单词；\n",
    "- 我们需要记录单词到index的mapping，index到单词的mapping，单词的count，单词的(normalized) frequency以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file='./text8/text8.train.txt', mode='r') as fin:\n",
    "    text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本中读取所有的文字，分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [w for w in word_tokenize(text=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过这些文本创建一个vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<unk>'] = len(text) - np.sum(list(vocab.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词汇的mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = [word for word in vocab.keys()]  # 取出所有单词表里的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('of', 1),\n",
       " ('and', 2),\n",
       " ('one', 3),\n",
       " ('in', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('zero', 7),\n",
       " ('nine', 8),\n",
       " ('two', 9)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每一个单词的frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = word_counts / np.sum(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_counts / np.sum(word_counts)  # 用来做negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看预处理之后的词汇数是否有MAX_VOCAB_SIZE那么多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(idx_to_word); VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们在训练的时候，我们需要一个batch一个batch的数据，我们可以使用Pytorch实现一个Dataloader，需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字；\n",
    "- 保存vocabulary，单词count，normalized word frequency；\n",
    "- 每个iteration sample一个中心词；\n",
    "- 根据当前的中心词返回context单词；\n",
    "- 根据当前中心词sample一些negative单词；\n",
    "- 返回单词的counts\n",
    "\n",
    "为了使用Dataloader，我们需要定义一下两个function：\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item；\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "    \n",
    "有了Dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset):  # 继承tud.Dataset父类\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        super(WordEmbeddingDataset, self).__init__()  # 初始化模型\n",
    "        self.text_encoded = [word_to_idx.get(word, word_to_idx['<unk>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word_to_idx = word_to_idx  # 保存数据\n",
    "        self.idx_to_word = idx_to_word  # 保存数据\n",
    "        self.word_freqs = torch.Tensor(word_freqs)  # 保存数据\n",
    "        self.word_counts = torch.Tensor(word_counts)  # 保存数据\n",
    "    \n",
    "    def __len__(self):  # 数据集有多少个item\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, idx):  # 魔法函数__getitem__为迭代器，返回以下数据用于训练\n",
    "        \"\"\"\n",
    "            - 中心词center_word\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        \"\"\"\n",
    "        center_word = self.text_encoded[idx]  # idx代表了所有单词索引\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))  # 周围词索引\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], replacement=True)  # 负例采样单词索引，torch.multinomial()作用是对self.word_freqs做K*pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标\n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Dataset和Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(\n",
    "    text=text,\n",
    "    word_to_idx=word_to_idx,\n",
    "    idx_to_word=idx_to_word,\n",
    "    word_freqs=word_freqs,\n",
    "    word_counts=word_counts\n",
    ")\n",
    "dataloader = tud.DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1bcf466aeb8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  444,  1228,   275, 24695, 29999,    51,     0,     2,    24,    16,\n",
       "             9,  2120,  2128, 29999,    31,   544,     0,   359,  2225,   641,\n",
       "            10,  5100,   106,  9018,    14,     0, 20506,  2728,  1015,    33,\n",
       "          9262,   201,  7064,    80,  1210, 18818,  2131,  3345,    26,  7813,\n",
       "           980,   257,    19,    24,  5172,  3779,    64,     1,     5,     1,\n",
       "          1512,   126,    26,    10,     6,   144, 29999,     5,    68,    27,\n",
       "            20,     0,  3241,   564,   699,     4,    71, 13595,   296, 29999,\n",
       "           257,  3921,  2047, 29999,     0,  6222,     3,    19,  6167,     4,\n",
       "            17,     1,   238,  2388,     1, 21971,   372, 29999,   155,  3444,\n",
       "            26,    14,    71,   452,   359,     9,     5, 29999,  3259,   409,\n",
       "           610,     5,    49,    71,    35,     3,  5816, 29999,    94,  2243,\n",
       "            73,  4319,    36,    10,    42,   136,  1500,  2725,   185,     5,\n",
       "           764,   238,  4301,  8969, 29999,  1009,     7,     0]),\n",
       " tensor([[  299,     6,    32,     0,   621,    14],\n",
       "         [   71,  2253,   190,    34,   343,   411],\n",
       "         [    6,   495,   180,     6,     0,  4094],\n",
       "         [ 2362, 14933,     2,   118,  5055,    17],\n",
       "         [   54,    11, 29999, 29999,  4351,  9763],\n",
       "         [  392,   182,     2,    31,  3809,     4],\n",
       "         [  319,  3606,  3156, 21896,     1,   177],\n",
       "         [    0, 26231,  1433,     5,   218,  2423],\n",
       "         [ 2703,     1,  1990,   146,    14,  2640],\n",
       "         [    3,     8,    15,    10,    37,    30],\n",
       "         [   11,   394,    11,    15,     1,  1232],\n",
       "         [   14,    52,  4620,  2650,  4723,    23],\n",
       "         [  334,  7179,    16,     1,  1039, 13471],\n",
       "         [17577,     2,     0,    24,     0,   538],\n",
       "         [   10,   724,     6,     5,   114, 29999],\n",
       "         [  570,     0,  3513,    51,    31,  4943],\n",
       "         [  729,    10,    36,   729,     1,  4096],\n",
       "         [   38,     2,    40,    48, 15728,     6],\n",
       "         [ 2950,    13,  6038,   236,     4,     0],\n",
       "         [  136,    49,   257,  2810,     4,  2910],\n",
       "         [ 1653,     1,    26,   305,     2,    16],\n",
       "         [ 6036,    11,     0,   270,   342,   605],\n",
       "         [15568,  7316,   541,  1743,     4,     0],\n",
       "         [   10,     0,  2259,  2761,     7,    10],\n",
       "         [    4,     0,   882,  3437,    11,   110],\n",
       "         [    8,    21,    16,  3802, 12788,     3],\n",
       "         [29999,     1, 16257,  1773,   130,     5],\n",
       "         [  855,     6,     0,    47,     1,     0],\n",
       "         [  598,  6693,   208,  2183,  9235,     0],\n",
       "         [    1, 29999,     2,    39,   325,    11],\n",
       "         [   14,   911,  3171,     0,   234,    57],\n",
       "         [  184,   344,     2,  2650,     4,   508],\n",
       "         [   13,  7221,    56,    13, 29999,     4],\n",
       "         [    1,     0,    87,     6,  1305,     0],\n",
       "         [ 3460,     1,     0,     4,     0,   270],\n",
       "         [   49,  7372, 12581,     2, 10027, 19973],\n",
       "         [    9,     3,   655,  1173, 17679,    19],\n",
       "         [   23,  2756,     2, 29999,   106,   220],\n",
       "         [  809,  1474,    11,    72,  1903,     0],\n",
       "         [  233,  1583,    42,     1,  1739, 11643],\n",
       "         [  413,  1576,     4,     2,    19,    45],\n",
       "         [    1,  2254,  1181,   641,     4,   257],\n",
       "         [ 2171,    23,  2957,    25,   231,     6],\n",
       "         [   17,  2123,  1323, 11146, 11816,   329],\n",
       "         [    2, 10500, 12524,    28, 29999,  1128],\n",
       "         [   19, 29999,     0,  1524,     0,  2107],\n",
       "         [ 1383,     6,   680,     9,  7908,    25],\n",
       "         [    4,     0,  4306,     0, 25888,     1],\n",
       "         [  851,     6, 17377,  1485,  4201,  1400],\n",
       "         [  195,     0, 21869,  9534,    32,    17],\n",
       "         [    1,  3500, 29999,  3696,   866, 11679],\n",
       "         [   39,  2154,    18,   113,     9,     7],\n",
       "         [  871, 10100,   286,   416,     0,   594],\n",
       "         [ 4184,     0,   825,  9452,    27, 29999],\n",
       "         [  161,    31,    59, 12580,  1246,  6632],\n",
       "         [  443,   988,     0,   443,  2253,   176],\n",
       "         [ 2854,    23,   158,     2,   379,   803],\n",
       "         [    5, 13922,     2,   318,  7934,   858],\n",
       "         [ 1492,    46,    28,  2438,    33,   198],\n",
       "         [   76, 16098,  2067,     0,   585,    19],\n",
       "         [    3,     8,     9,     3,     8,    16],\n",
       "         [  903,  1165,    18,  4593,   406,     5],\n",
       "         [ 2465,   825, 15043,   825, 15043,   431],\n",
       "         [ 5860, 29999,    45,     0,   443,  2253],\n",
       "         [    2,     0, 29999,  2433,    18,     0],\n",
       "         [ 3691,     6,   178,   116,   989, 12162],\n",
       "         [    1,     0,   190,    23,   190,   626],\n",
       "         [29999, 15300,    17,    60,  4242,   120],\n",
       "         [  881,   720,    23,  3091,     4,  1083],\n",
       "         [   14,   203,     4,     0,  3554,   237],\n",
       "         [   34,     0,    71,   641,  1147,     4],\n",
       "         [   12,    15,   431,  4734, 29999,  1896],\n",
       "         [    6,  2686,     1,    28,    30,  7054],\n",
       "         [ 4067, 24610,  1718,    75,     3,     8],\n",
       "         [  526,   380,  3341,   966,    17,  6944],\n",
       "         [15007,    61,   474,     6,  2261,   115],\n",
       "         [  459,     9,    47,    15,     7,     7],\n",
       "         [ 1034,    32,   355,  2148,  4259,   167],\n",
       "         [  731,   263,    36,   250,     6, 14050],\n",
       "         [  177,     6, 21984,     0, 22000, 13956],\n",
       "         [    0,  8554,     2,   197,     4,     5],\n",
       "         [14050,     3,  1213,     0, 29999,   343],\n",
       "         [ 3593,    26,    10,    19,     5, 21720],\n",
       "         [  423,   140,    64,   423,    39,    36],\n",
       "         [    0, 12296,  2016,    14,   230, 29999],\n",
       "         [16751,   451,   645,    13,   246, 12792],\n",
       "         [    0,    82,    14,     1,   501,    80],\n",
       "         [  211,     1,  1463,  3372,  1677, 29999],\n",
       "         [  449,  6070,  1724,    94,  2838,   567],\n",
       "         [    5,  4953,     1,  1473,    25,    34],\n",
       "         [ 4337,  9068,   585,    48,    53,   970],\n",
       "         [   11,  2356, 29999,     0, 29999,     1],\n",
       "         [ 5366, 29999,     0,  1371,  3177,     1],\n",
       "         [    0,   254,   337, 18939,    24,    32],\n",
       "         [ 1425,   682,    72,    31,  5026, 29999],\n",
       "         [   12,    12,   426,     7,     7,     7],\n",
       "         [   14,   431,    11,  3288,   100,  2080],\n",
       "         [29999,  1345, 29999,   968, 29999,    14],\n",
       "         [   52,     1,     0,    93,   484,   970],\n",
       "         [ 4397,     0, 26894,     4,  3166,  3163],\n",
       "         [16867,   283,    11,  1899,  7234,    39],\n",
       "         [    6,    38,    53, 29999,    76,    23],\n",
       "         [  279,     6,   138,  3378,  2823,     0],\n",
       "         [29999,   583,     0,  1171,  9220, 29999],\n",
       "         [  268,  1281, 29999,  6920,    44, 14649],\n",
       "         [    3,     9,     7,     9,     9,     3],\n",
       "         [14261,  5363,    10,    18,  2188,  7967],\n",
       "         [  116,   353, 24499,    17,   508,    14],\n",
       "         [  158,   301, 29999,   158,    81,    22],\n",
       "         [   14, 14428,    14,   164,   333,   813],\n",
       "         [ 5162,   643,    29,    14,     4,  1907],\n",
       "         [   19,    50,   752,     4, 26405,    28],\n",
       "         [ 2754,    32,    10,     5,   458,  2126],\n",
       "         [  190, 29999, 27530,     5,  1925,     1],\n",
       "         [21096,  4361,     2, 18395, 14740,    54],\n",
       "         [    8,    21,    21, 29999,     4, 11326],\n",
       "         [29999,    27,    29,    61,    35,  1634],\n",
       "         [15905,   444,     4,     3,    15,     9],\n",
       "         [  162,  4399,   139,   684,     4,   508],\n",
       "         [11550, 29999, 17392,  2794, 22936,     4],\n",
       "         [  686,     6,     0,   636,     2,   101],\n",
       "         [   13,    50,    29,  6372,  7168,    17],\n",
       "         [ 2255,     0,   181,  2020,  2343,     5],\n",
       "         [18095,  3820,   528,    32,  6228,   588],\n",
       "         [  122, 20259, 13518,  1306,   950,     1],\n",
       "         [    2,  3037,    27,    24,   826,   967],\n",
       "         [   20,    15,     9,   196,   734,     3],\n",
       "         [  164,  6251,     1,   207,   512,    26]]),\n",
       " tensor([[  653,     1,    72,  ...,   373,   205,  2044],\n",
       "         [   24,     1,  9050,  ...,     2,  1248,   891],\n",
       "         [ 1396, 22942,     5,  ...,   335,     3,   879],\n",
       "         ...,\n",
       "         [   13,  1403, 23708,  ...,    46,  4587,     5],\n",
       "         [  191,     9,    53,  ...,     4,  8857,   356],\n",
       "         [  151,     4,     5,  ...,  5192,    17,   506]])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 定义PyTorch模型\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        init_range = 0.5 / self.embed_size\n",
    "        self.in_embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embed_size, sparse=False)\n",
    "        self.in_embed.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out_embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embed_size)\n",
    "        self.out_embed.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_lables):  # loss function\n",
    "        \"\"\"\n",
    "        :param input_labels: [batch_size]\n",
    "        :param pos_labels: [batch_size, (window_size * 2)]\n",
    "        :param neg_lables: [batch_size, (window_size * 2 * K)]\n",
    "        :return: loss, [batch_size]\n",
    "        \"\"\"\n",
    "        batch_size = input_labels.size(0)\n",
    "\n",
    "        input_embedding = self.in_embed(input_labels)  # [batch_size, embed_size]\n",
    "        pos_embedding = self.out_embed(pos_labels)  # [batch_size, (window_size * 2), embed_size]\n",
    "        neg_embedding = self.out_embed(neg_lables)  # [batch_size, (window_size * 2 * K), embed_size]\n",
    "\n",
    "        # unsqueeze()升维, squeeze()降维\n",
    "        input_embedding = input_embedding.unsqueeze(2)  # [batch_size, embed_size, 1], 第二个维度加1\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding).squeeze()  # [batch_size, (window_size * 2)]\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding).squeeze()  # [batch_size, (window_size * 2 * K)]\n",
    "\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "        loss = log_pos + log_neg\n",
    "\n",
    "        return -loss\n",
    "\n",
    "    def input_embedding(self):  # 取出self.in_embed数据参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个模型以及把模型移动到GPU\n",
    "model = EmbeddingModel(vocab_size=VOCAB_SIZE, embed_size=EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "# 评估模型\n",
    "def evaluate(filename, embedding_weights):\n",
    "    if filename.endswith('.csv'):\n",
    "        data = pd.read_csv(filename, sep=',')\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep='\\t')\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)\n",
    "\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "\"\"\"\n",
    "1.模型一般需要训练若干个epoch，每个epoch我们都把所有数据分成若干个batch，把每个batch的输入和输出都包装成cuda tensor；\n",
    "2.forward pass，通过输入的句子预测每个单词的下一个单词，用模型的预测和正确的下一个单词计算cross entropy loss；\n",
    "3.清空模型当前的Gradient；\n",
    "4.backward pass，更新模型参数；\n",
    "5.每隔一定的iteration，输出模型在当前iteration的loss以及在验证数据集上做模型的评估。\n",
    "\"\"\"\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=LEARNING_RATE)\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataLoader):\n",
    "        # print(input_labels, pos_labels, neg_labels)\n",
    "        # if i > 2:\n",
    "        #     break\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_labels, pos_labels, neg_labels).mean()  # 传入参数给forward()函数\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                with open(file=LOG_FILE, mode='a', encoding='UTF-8') as f_out:\n",
    "                    f_out.write('Epoch: {}, Iteration: {}, Loss: {} + \\n'.format(e, i, loss.item()))\n",
    "                    print(f'Epoch: {e}, Iteration: {i}, Loss: {loss.item()}')\n",
    "\n",
    "            if i % 2000 == 0:\n",
    "                embedding_weights = model.input_embedding()\n",
    "                sim_simlex = evaluate(filename='simlex-999.txt', embedding_weights=embedding_weights)\n",
    "                sim_men = evaluate(filename='men.txt', embedding_weights=embedding_weights)\n",
    "                sim_353 = evaluate(filename='wordsim353.csv', embedding_weights=embedding_weights)\n",
    "                with open(file=LOG_FILE, mode='a') as f_out:\n",
    "                    print(f'Epoch: {e}, Iteration: {i}, sim_simlex: {sim_simlex}, sim_men: {sim_men}, sim_353: {sim_353}, nearest to monster: {find_nearest(word=\"monster\")} + \\n')\n",
    "                    f_out.write('Epoch: {}, Iteration: {}, sim_simlex: {}, sim_men: {}, sim_353: {}, nearest to monster: {} + \\n'.format(\n",
    "                        e, i, sim_simlex, sim_men, sim_353, find_nearest(word=\"monster\")))\n",
    "\n",
    "    embedding_weights = model.input_embedding()\n",
    "    np.save('embedding-{}'.format(EMBEDDING_SIZE), embedding_weights)\n",
    "    torch.save(model.state_dict(), 'embedding-{}.th'.format(EMBEDDING_SIZE))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Torch10] *",
   "language": "python",
   "name": "conda-env-Torch10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
