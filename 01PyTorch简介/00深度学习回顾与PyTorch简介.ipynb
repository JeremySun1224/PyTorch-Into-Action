{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595403320952",
   "display_name": "Python 3.6.9 64-bit ('Torch10': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'1.0.1'"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'GeForce GTX 1080 Ti'"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 什么是PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch是一个基于Python的科学计算库，类似于NumPy，但是它可以使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor类似于NumPy的ndarray，唯一的区别是Tensor可以在GPU上进行加速运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个未初始化的5*3矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-1.4197e-37,  7.5530e-43, -7.3176e-38],\n        [ 7.5530e-43, -7.3308e-38,  7.5530e-43],\n        [-7.3311e-38,  7.5530e-43, -7.3312e-38],\n        [ 7.5530e-43, -7.3312e-38,  7.5530e-43],\n        [-7.3313e-38,  7.5530e-43, -7.3313e-38]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x = torch.empty(5, 3); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建随机初始化矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.9758, 0.1649, 0.8757],\n        [0.9409, 0.1528, 0.3944],\n        [0.5435, 0.4614, 0.7119],\n        [0.4058, 0.9775, 0.6182],\n        [0.1247, 0.6225, 0.6064]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "x = torch.rand(5, 3); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个全部为0，类型为long的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x = torch.zeros(5, 3); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.float32"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.int64"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从数据直接构建Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([5.5000, 3.0000])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3]); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[2, 3],\n        [5, 9]])"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [\n",
    "        [2, 3], \n",
    "        [5, 9]\n",
    "    ]\n",
    "); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从已有Tensor构建一个Tensor会重用原来Tensor的特征。例如，数据类型等。除非提供新的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1],\n        [1, 1, 1]])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "x = x.new_ones(5, 3); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.int64"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "x.dtype  # 可以看出新建的x重用了之前x的数据类型torch.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们也可以指定数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.double); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.float64"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重用数据形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.7226, 0.9590, 0.6864],\n        [0.1988, 0.8378, 0.4334],\n        [0.7738, 0.5135, 0.3211],\n        [0.6820, 0.9713, 0.2132],\n        [0.3054, 0.9054, 0.4199]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "y = torch.rand_like(x, dtype=torch.float64); y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到Tensor形状，注意torch.Size返回的是一个tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "type(x.shape)  # 是一个元组tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.7226, 1.9590, 1.6864],\n        [1.1988, 1.8378, 1.4334],\n        [1.7738, 1.5135, 1.3211],\n        [1.6820, 1.9713, 1.2132],\n        [1.3054, 1.9054, 1.4199]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.7226, 1.9590, 1.6864],\n        [1.1988, 1.8378, 1.4334],\n        [1.7738, 1.5135, 1.3211],\n        [1.6820, 1.9713, 1.2132],\n        [1.3054, 1.9054, 1.4199]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法3：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.7226, 1.9590, 1.6864],\n        [1.1988, 1.8378, 1.4334],\n        [1.7738, 1.5135, 1.3211],\n        [1.6820, 1.9713, 1.2132],\n        [1.3054, 1.9054, 1.4199]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "result = torch.empty(5, 3, dtype=torch.float64)\n",
    "torch.add(x, y, out=result)\n",
    "result  # 这样做可以减少内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加法4：in-place操作。任何in-place操作的运算都会以_结尾。举例来说x.copy_(y)，会改变x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1.7226, 1.9590, 1.6864],\n        [1.1988, 1.8378, 1.4334],\n        [1.7738, 1.5135, 1.3211],\n        [1.6820, 1.9713, 1.2132],\n        [1.3054, 1.9054, 1.4199]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "y.add_(x)  # 所有的下划线操作(in-place)是作用在y上的，就是会改变源y的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各种类似于NumPy的indexing操作也可以在PyTorch的Tensor上使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.1039, 0.9525, 0.8452],\n        [0.6057, 0.1973, 0.1643],\n        [0.2708, 0.0807, 0.4430],\n        [0.8523, 0.4176, 0.2398],\n        [0.7091, 0.6370, 0.4677]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "x = torch.rand_like(y, dtype=torch.float64); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0.9525, 0.8452],\n        [0.1973, 0.1643],\n        [0.0807, 0.4430],\n        [0.4176, 0.2398],\n        [0.6370, 0.4677]], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing: 如果希望resize/reshape一个Tensor，可以使用torch.view，在NumPy里是reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.3207,  0.6449, -0.4905,  1.3452],\n        [-1.3559,  0.3588,  1.8640,  0.9712],\n        [-0.3755,  1.3294, -0.1590, -0.1769],\n        [ 0.4891, -0.6439,  1.0613,  1.1723]])"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "x = torch.randn(4, 4); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.3207,  0.6449, -0.4905,  1.3452, -1.3559,  0.3588,  1.8640,  0.9712,\n        -0.3755,  1.3294, -0.1590, -0.1769,  0.4891, -0.6439,  1.0613,  1.1723])"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "x.view(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.3207,  0.6449, -0.4905,  1.3452],\n        [-1.3559,  0.3588,  1.8640,  0.9712],\n        [-0.3755,  1.3294, -0.1590, -0.1769],\n        [ 0.4891, -0.6439,  1.0613,  1.1723]])"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到view操作并不会改变源x的值，只有in-place才会改变源值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.3207,  0.6449, -0.4905,  1.3452, -1.3559,  0.3588,  1.8640,  0.9712],\n        [-0.3755,  1.3294, -0.1590, -0.1769,  0.4891, -0.6439,  1.0613,  1.1723]])"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "x.view(2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当只知道或只需要某一维的维度时，可以将另一维度写作-1，PyTorch会自动reshape这个Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.3207,  0.6449, -0.4905,  1.3452],\n        [-1.3559,  0.3588,  1.8640,  0.9712],\n        [-0.3755,  1.3294, -0.1590, -0.1769],\n        [ 0.4891, -0.6439,  1.0613,  1.1723]])"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.3207,  0.6449, -0.4905,  1.3452, -1.3559,  0.3588,  1.8640,  0.9712],\n        [-0.3755,  1.3294, -0.1590, -0.1769,  0.4891, -0.6439,  1.0613,  1.1723]])"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "x.view(-1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果是只有一个元素的Tensor，使用.item()方法可以把里面的value变成Python的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.1237])"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "x = torch.randn(1); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.12369722872972488"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.1237])"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch的操作，dir(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy和Tensor之间的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor转换为NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1., 1., 1., 1., 1.])"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "a = torch.ones(5); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "b = a.numpy(); b  # 这里的a和b共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 2., 1., 1., 1.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "b[1] = 2; b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1., 2., 1., 1., 1.])"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "a  # 可以看到a也改变了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy转换为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 1., 1., 1., 1.])"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "a = np.ones(5); a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "b = torch.from_numpy(a); b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2., 2., 2., 2., 2.])"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "np.add(a, 1, out=a); a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用.to()方法，可以把Tensor移到GPU device上进行加速计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0.1237], device='cuda:0') \n tensor([1.], device='cuda:0') \n tensor([1.1237], device='cuda:0')\n"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # 程序自动寻找Nvidia的GPU\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)  # 把Tensor搬到GPU上\n",
    "    z = x + y\n",
    "    print(x, '\\n', y, '\\n', z)\n",
    "    # print(z.to('cpu', torch.double))  # 把z搬回到CPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([1.1237], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # 程序自动寻找Nvidia的GPU\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)  # 把Tensor搬到GPU上\n",
    "    z = x + y\n",
    "    # print(x, '\\n', y, '\\n', z)\n",
    "    print(z.to('cpu', torch.double))  # 把z搬回到CPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果变量在GPU上，不能直接将其转换为CPU。需要先将其转到CPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把整个模型搬到GPU, model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 热身：用NumPy实现两层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model = Architecture + Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10  # 输入数据个数，输入维度，隐层维度，输出维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机创建训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 31466996.89913607\niter 1, loss 32718021.204082876\niter 2, loss 44043031.543425515\niter 3, loss 57686727.01357093\niter 4, loss 59058323.04549369\niter 5, loss 38507663.45951154\niter 6, loss 15450405.589939145\niter 7, loss 4814654.10642281\niter 8, loss 1920254.1931992732\niter 9, loss 1159827.4299147371\n"
    }
   ],
   "source": [
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # 1.forward pass\n",
    "    h = X.dot(w1)  # N * H\n",
    "    h_relu = np.maximum(h, 0)  # N * H\n",
    "    y_pred = h_relu.dot(w2)  # N * D_out\n",
    "    # 2.compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(f'iter {t}, loss {loss}')\n",
    "    # Backward pass\n",
    "    ## compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = X.T.dot(grad_h)\n",
    "    ## update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以看到loss在下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 34952572.0\niter 1, loss 35036756.0\niter 2, loss 41241688.0\niter 3, loss 44826480.0\niter 4, loss 38308968.0\niter 5, loss 23406632.0\niter 6, loss 10711394.0\niter 7, loss 4460092.0\niter 8, loss 2149122.5\niter 9, loss 1313488.625\n"
    }
   ],
   "source": [
    "X = torch.randn(N, D_in).cuda()  # 放在GPU上计算，device='cuda:0'\n",
    "# X = torch.randn(N, D_in).to('cuda:0')  # 指定device='cuda:0'\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "w1 = torch.randn(D_in, H).cuda()\n",
    "w2 = torch.randn(H, D_out).cuda()\n",
    "\n",
    "# print(f'X: {X}, y: {y}, w1: {w1}, w2: {w2}')\n",
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # 1.forward pass\n",
    "    h = X.mm(w1)  # N * H\n",
    "    h_relu = h.clamp(min=0)  # N * H\n",
    "    y_pred = h_relu.mm(w2)  # N * D_out\n",
    "    # 2.compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(f'iter {t}, loss {loss}')\n",
    "    # Backward pass\n",
    "    ## compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = X.t().mm(grad_h)\n",
    "    ## update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的autograd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(5., grad_fn=<AddBackward0>)"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "y = w * x + b; y  # y = 2 * 1 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果要求y关于x的梯度，可以直接使用y.backward()自动求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "w.grad, 1.0\nx.grad, 1.0\nb.grad, 1.0\n"
    }
   ],
   "source": [
    "print(f'w.grad, {w.grad}')\n",
    "print(f'x.grad, {w.grad}')\n",
    "print(f'b.grad, {w.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensor和autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 需要把数据、损失函数和模型等都放入GPU中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch的一个重要功能是autograd，也就是说只要定义了forward pass（前向神经网络），计算了loss后，PyTorch可以自动求导计算模型所有参数的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个PyTorch的Tensor表示计算图中的一个节点。如果x是一个Tensor并且x.requires_grad=True，那么x.grad是一个另一个储存着x当前梯度（相对于一个scalar，常常是loss）的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\niter 0, loss 31492254.0\niter 1, loss 28703134.0\niter 2, loss 31292642.0\niter 3, loss 34162792.0\niter 4, loss 32454456.0\niter 5, loss 24952252.0\niter 6, loss 15218750.0\niter 7, loss 7961044.5\niter 8, loss 4048104.5\niter 9, loss 2262352.0\n"
    }
   ],
   "source": [
    "X = torch.randn(N, D_in).cuda()  # 放在GPU上计算，device='cuda:0'\n",
    "# X = torch.randn(N, D_in).to('cuda:0')  # 指定device='cuda:0'\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "w1 = torch.randn(D_in, H, requires_grad=True, device=device)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True, device=device)\n",
    "print(y.is_leaf)\n",
    "# print(f'X: {X}, y: {y}, w1: {w1}, w2: {w2}')\n",
    "learning_rate = 1e-6\n",
    "for t in range(10):\n",
    "    # 1.forward pass\n",
    "    # h = X.mm(w1)  # N * H\n",
    "    # h_relu = h.clamp(min=0)  # N * H\n",
    "    # y_pred = h_relu.mm(w2)  # N * D_out\n",
    "    y_pred = X.mm(w1).clamp(min=0).mm(w2).cuda()\n",
    "    # 2.compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().cuda()\n",
    "    print(f'iter {t}, loss {loss}')\n",
    "    # Backward pass\n",
    "    ## compute the gradient\n",
    "    # grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    # grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    # grad_h = grad_h_relu.clone()\n",
    "    # grad_h[h < 0] = 0\n",
    "    # grad_w1 = X.t().mm(grad_h)\n",
    "    loss.backward()\n",
    "    ## update weights of w1 and w2\n",
    "    # w1 -= learning_rate * w1.grad\n",
    "    # w2 -= learning_rate * w2.grad\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "w1, torch.Size([1000, 100]), w2, torch.Size([100, 10])\n"
    }
   ],
   "source": [
    "X = torch.randn(N, D_in).cuda()  # 放在GPU上计算，device='cuda:0'\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True, device=device)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True, device=device)\n",
    "\n",
    "y_pred = X.mm(w1).clamp(min=0).mm(w2).cuda()\n",
    "loss = (y_pred - y).pow(2).sum().cuda()\n",
    "# print(f'iter {t}, loss {loss}')\n",
    "# for i in range(5):\n",
    "loss.backward()\n",
    "w1 = w1 - learning_rate * w1.grad\n",
    "w2 = w2 - learning_rate * w2.grad\n",
    "print(f'w1, {w1.shape}, w2, {w2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 放到GPU上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 40641304.0\niter 1, loss 44717200.0\niter 2, loss 52143172.0\niter 3, loss 50191388.0\niter 4, loss 33907368.0\niter 5, loss 15680492.0\niter 6, loss 6101679.0\niter 7, loss 2793259.0\niter 8, loss 1728043.875\niter 9, loss 1294621.0\n"
    }
   ],
   "source": [
    "X = torch.randn(N, D_in).cuda()\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "# device = torch.cuda.device('cuda:0')，这里的错误写法\n",
    "device = torch.device('cuda:0')\n",
    "w1 = torch.randn(D_in, H, requires_grad=True, device=device)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(10):\n",
    "    # 1.forward pass\n",
    "    y_pred = X.mm(w1).clamp(min=0).mm(w2).cuda()\n",
    "    # 2.compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().cuda()\n",
    "    print(f'iter {it}, loss {loss}')\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # update weights of w1 and w2\n",
    "    # 为了不让计算图占内存，使用torch.no_grad()，就不会记住w1和w2的梯度。\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在CPU上运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "it: 0, loss: 28325130.0\nit: 1, loss: 24314682.0\nit: 2, loss: 23358280.0\nit: 3, loss: 22278968.0\nit: 4, loss: 19547522.0\nit: 5, loss: 15201494.0\nit: 6, loss: 10530118.0\nit: 7, loss: 6716563.5\nit: 8, loss: 4143936.0\nit: 9, loss: 2592636.25\n"
    }
   ],
   "source": [
    "X = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(10):\n",
    "    # Forward pass\n",
    "    y_pred = X.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()  # computation graph\n",
    "    print(f'it: {it}, loss: {loss}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 46458704.0\niter 1, loss 46189048.0\niter 2, loss 41337848.0\niter 3, loss 28393146.0\niter 4, loss 14855011.0\niter 5, loss 6812826.0\niter 6, loss 3406305.5\niter 7, loss 2061754.75\niter 8, loss 1460919.375\niter 9, loss 1128984.5\n"
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10  # 输入数据个数，输入维度，隐层维度，输出维度\n",
    "\n",
    "X = torch.randn(N, D_in).cuda()\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# w1 = torch.randn(D_in, H, requires_grad=True, device=device)\n",
    "# w2 = torch.randn(H, D_out, requires_grad=True, device=device)\n",
    "\n",
    "# model里面只是控制数据输入输出维度的，在定义阶段并不需要传入数据\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=D_in, out_features=H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=H, out_features=D_out)\n",
    ")\n",
    "model = model.cuda(device=device)\n",
    "\n",
    "# 初始化线性层参数\n",
    "nn.init.normal_(model[0].weight)\n",
    "nn.init.normal_(model[2].weight)\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "loss_fn = loss_fn.cuda(device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(10):\n",
    "    # 1.forward pass\n",
    "    # y_pred = X.mm(w1).clamp(min=0).mm(w2).cuda()\n",
    "    y_pred = model(X)  # model.forward()\n",
    "    # 2.compute loss\n",
    "    # loss = (y_pred - y).pow(2).sum().cuda()\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(f'iter {it}, loss {loss}')\n",
    "\n",
    "    model.zero_grad()  # 清零所有参数梯度\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # update weights of w1 and w2\n",
    "    # 为了不让计算图占内存，使用torch.no_grad()，就不会记住w1和w2的梯度。\n",
    "    # with torch.no_grad():\n",
    "    #     w1 -= learning_rate * w1.grad\n",
    "    #     w2 -= learning_rate * w2.grad\n",
    "    #     w1.grad.zero_()\n",
    "    #     w2.grad.zero_()\n",
    "    with torch.no_grad():\n",
    "        # model.parameters()里有很多参数的梯度，如w1,w2, b1, b2等，\n",
    "        for param in model.parameters():  # 所有模型的参数都在param里\n",
    "            param -= learning_rate * param.grad\n",
    "    # model.zero_grad()  # 清零所有参数梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=1000, out_features=100, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=100, out_features=10, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Linear(in_features=1000, out_features=100, bias=True)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.0088,  0.0025,  0.0292,  ...,  0.0216, -0.0028,  0.0183],\n        [ 0.0043,  0.0249,  0.0010,  ...,  0.0078,  0.0153,  0.0194],\n        [-0.0021, -0.0241,  0.0038,  ...,  0.0154, -0.0248,  0.0095],\n        ...,\n        [ 0.0286,  0.0169, -0.0184,  ...,  0.0045,  0.0137, -0.0122],\n        [-0.0193,  0.0260,  0.0232,  ...,  0.0239, -0.0077,  0.0001],\n        [-0.0168,  0.0044, -0.0077,  ...,  0.0120, -0.0268, -0.0010]],\n       device='cuda:0', requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这一次我们不再手动更新模型的weights，而是使用optim这个包来帮助我们更新参数。optim这个package提供了各种不同的模型优化方法，包括SGD+momentum，RMSProp，Adam等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 30414648.0\niter 1, loss 29018474.0\niter 2, loss 32293482.0\niter 3, loss 34286220.0\niter 4, loss 30563664.0\niter 5, loss 21128370.0\niter 6, loss 11583691.0\niter 7, loss 5615385.0\niter 8, loss 2824090.75\niter 9, loss 1642769.625\n"
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10  # 输入数据个数，输入维度，隐层维度，输出维度\n",
    "\n",
    "X = torch.randn(N, D_in).cuda()\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# model里面只是控制数据输入输出维度的，在定义阶段并不需要传入数据\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=D_in, out_features=H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=H, out_features=D_out)\n",
    ")\n",
    "model = model.cuda(device=device)\n",
    "\n",
    "# 初始化线性层参数\n",
    "nn.init.normal_(model[0].weight)\n",
    "nn.init.normal_(model[2].weight)\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "loss_fn = loss_fn.cuda(device=device)\n",
    "\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(10):\n",
    "    # 1.forward pass\n",
    "    y_pred = model(X)  # model.forward()\n",
    "    \n",
    "    # 2.compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(f'iter {it}, loss {loss}')\n",
    "\n",
    "    # model.zero_grad()  # 清零所有参数梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update model parameters\n",
    "    optimizer.step()  # 一步更新所有的参数\n",
    "\n",
    "# 所以我们要做的就是：定义一个optimizer，求导之前将梯度清空，求导滞后将梯度更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch：自定义nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更复杂的模型，就需要自定义nn.Module模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "iter 0, loss 619.3310546875\niter 1, loss 602.7033081054688\niter 2, loss 586.5711669921875\niter 3, loss 570.8697509765625\niter 4, loss 555.6207885742188\niter 5, loss 540.8893432617188\niter 6, loss 526.600341796875\niter 7, loss 512.7101440429688\niter 8, loss 499.1954650878906\niter 9, loss 486.0283508300781\n"
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10  # 输入数据个数，输入维度，隐层维度，输出维度\n",
    "\n",
    "X = torch.randn(N, D_in).cuda()\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# model里面只是控制数据输入输出维度的，在定义阶段并不需要传入数据\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(in_features=D_in, out_features=H),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(in_features=H, out_features=D_out)\n",
    "# )\n",
    "\n",
    "# 要想写出更复杂的模型，我们需要继承nn.Module\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = nn.Linear(in_features=D_in, out_features=H)\n",
    "        self.linear2 = nn.Linear(in_features=H, out_features=D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model = model.cuda(device=device)\n",
    "\n",
    "# # 初始化线性层参数\n",
    "# nn.init.normal_(model[0].weight)\n",
    "# nn.init.normal_(model[2].weight)\n",
    "\n",
    "# 损失函数\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "loss_fn = loss_fn.cuda(device=device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(10):\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(f'iter {it}, loss {loss}')\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ]
}